[{"content":" Designed By CEMTREX Product Description Its a user friendly and portable BCI system that acts as a EEG acquisition unit plus a Brain control device. This device tracks the state of mind and accordingly help the user to into proper state by guiding him/her using musical feedback and thus train the person to overcome some brain related disorders.\nAnother application is to control external IoT devices and mobile applications using intentionally generated neurophysiological signals i.e. controlling the application using brainwave. The device can also be used as a normal headphone.\nWorking The device has three electrode which pressed on user head to capture the brain signals. The small EEG signals are amplified by a precision amplifier and filtered electronically. The analog data is converted into digital data using a 16 bit ADC IC. The digital data is transmitted to the PC over the bluetooth. A microcontrller is used to interface with the ADC IC and bluetooth module.\nThe RAW brainwave data is received on the PC and processed and filtered using the software algorithm. The algorithm converts the RAW brain data into different EEG bands (Alpha, Beta, Delta). The algorithm analyze the EEG data and indicates the different state of mind such as Attention, Meditation. These parameters can be used to control the IoT devices.\nFor the Demo, A Wireless Car is designed which only starts when the user focus on anything like reading the brain changes the pattern of alpha wave. Check the demo video below.\nEEG RAW data Initial 3D Printed Prototype Demo of Controlling wireless Car using Brainwave Headset ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/brainwave-headset/","summary":"Designed By CEMTREX Product Description Its a user friendly and portable BCI system that acts as a EEG acquisition unit plus a Brain control device. This device tracks the state of mind and accordingly help the user to into proper state by guiding him/her using musical feedback and thus train the person to overcome some brain related disorders.\nAnother application is to control external IoT devices and mobile applications using intentionally generated neurophysiological signals i.","title":"Brainwave Headset"},{"content":" Client CEMTREX Product Description Working Product Specifications Product Photos/Videos ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/smartdesk/","summary":" Client CEMTREX Product Description Working Product Specifications Product Photos/Videos ","title":"SmartDesk"},{"content":" Client Product Description The Product is used in the \u0026lt;strong\u0026gt;EXIDE\u0026lt;/strong\u0026gt; battery manufacturing plant to measure and log the voltages of the Lead Acid batteries while its manufacturing process. The product panel is divided into 48 datalogging cards and each card can measure and log the voltages of 20 lead acid batteries connected in series. So each panel can log the data of 960 batteries. The recorded data is stored in the card and All cards data stored in the MySQL database (Windows Machine). Each card is connected to a 24 channel Ethernet switch and the ethernet switch is connected to the local computer for the connectivity.\nProduct Architecture Working In the battery manufacturing process the battery is charged for 48 hours by a rectifier and after particular interval the battery voltages needs to be measure to check if the batteries are charging accordingly to the charge cycle.the Card has two trigger inputs which is connected to the rectifier. once the trigger comes from the rectifier its log the data of that time. Windows machine runs a MySQL database server to store all the logs and trigger data from the card and Custom designed software is installed to read/display all the logged voltages.\nOnce the windows server comes online the card transfers all the locally stored data (JSON) into the MySQL database by executing the SQL quires. The software is also used to add a new card, set the trigger interval,create a batch of batteries. Each card is identified by its IP address.\nThe card has a overvolatge detection feature that detects if any battery voltages goes above the the threshold voltage limit. If the overvoltage condition occures then the buzzer and Alert indicator on the card turn ON.\nProduct Specifications 20 Voltage measurement channels. Voltage measurement range: 0V - 48V DC Voltage measuring accuracy: +/-50mV 2 Trigger Channels. TCP/IP connection with server (over Ethernet) 4Gb onboard storage memory. 12V Card Input voltage LED indications for card power, Server live, logging, Alert Onboard RTC (Real Time Clock). Battery Overvoltage detection. (Each channel is ) Interactive Windows software for live voltage diaplay, Displaying stored records and Report Generation. Single Server/Software supports upto 7 Panels. (Total 6720 battries) Product Photos/Videos Datalogger Software UI\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/20-channel-voltage-datalogger/","summary":"Client Product Description The Product is used in the \u0026lt;strong\u0026gt;EXIDE\u0026lt;/strong\u0026gt; battery manufacturing plant to measure and log the voltages of the Lead Acid batteries while its manufacturing process. The product panel is divided into 48 datalogging cards and each card can measure and log the voltages of 20 lead acid batteries connected in series. So each panel can log the data of 960 batteries. The recorded data is stored in the card and All cards data stored in the MySQL database (Windows Machine).","title":"20 Channel Voltage Data Logger"},{"content":"\rBluetooth Module Bluetooth modules are used to trammit/recive the data over the wiress connection between the two device. It is SiP (System in package) with bluetooth SoC with other passive components like crystal,memory,votage regulators, RF anteena. The modules ared desgined for fast product developemt. They are pre certified so it saves the time and cost of certification and easy to use in the product. Modules also comes with the liceses like APTX so there is no need to purchase it.\nDetailed information on Bluetooth Module Complete Guide on Bluetooth Module Bluetooth Chipset vs Module List of Bluetooth Modules Used/Tested Audio Bluetooth Modules 1. Sierra Wireless BC127 Bluetooth 4.2 Module Transmit/Receive High quality audio data BLE supported. Supports audio profiles(A2DP,AVRCP,HFP,MAP) Used in BlueBox project. Detailes functions\n2. Microchip RN52 Bluetooth 3.0 Supported audio profiles(A2DP,AVRCP,HFP,SPP) Suports iAP profile (iPod Accessory Protocol) Used for receving Audio USed in bluetooth speaker product (A2DP profile) USed in BrainWave headset project. (A2DP profile) 3. Feasycom FSC-BT966 Bluetooth 5.0 Supports BR/EDR and LE audio and data commumication Support bluetooth profiles (SPP, BLE, HFP, AVRCP, A2DP, HID) Used to send mic audio directly to bluetooth speaker. (Used as a audio trasmitter) (HFP,A2DP profile) 4. Feasycom FSC-BT806A Bluetooth 5.1 Audio trasmitter/reciver Support bluetooth profiles (SPP, BLE, HFP, AVRCP, A2DP, HID) suppports aptX, apt-X LL aptx is a Qualcomm high quality sound trsamfer. apt-x LL is a low latency protocol. Used to send mic audio direeclty to bluetooth speaker with low latency. (HFP,A2DP profile with aptx-LL) 5. HM10 Bluetooth 4.0 BLE Supported (Bluetooth Low PowerEnergy) Used to trasnfer the data in BLE protocl. Beacon mode. Used in Nulock project 6. HC05 Bluetooth 2.0 supports EDR (Enhanced data rate) Used to trammit and receive serial data in the products (SPP Profile) 7. ESP32 PCB Samples ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/modules/bluetooth/","summary":"Bluetooth Module Bluetooth modules are used to trammit/recive the data over the wiress connection between the two device. It is SiP (System in package) with bluetooth SoC with other passive components like crystal,memory,votage regulators, RF anteena. The modules ared desgined for fast product developemt. They are pre certified so it saves the time and cost of certification and easy to use in the product. Modules also comes with the liceses like APTX so there is no need to purchase it.","title":"Bluetooth Modules"},{"content":"\rDescription USB-C is a 24-pin USB connector system with a rotationally symmetrical connector. Its symmetrical design means it can be inserted either way up or down. The USB C port can be used for video adaptors Alternate mode. It enables adapters to output video from that same USB-C port to HDMI, DisplayPort, VGA, and other types of video connectors. It also used for USB Power Delivery (USB PD) to charger/power the devices. It is supports the Intel\u0026rsquo;s Thunderbolt 3 data transfer technology.\nWorking This project implemented a USB3.2 HUB with one Type C downstream port and Type A downstream port. Type C cable used for upstream connection. The VL822 used as a USB3.2 hub controller. Detailed description of VL822 USB3.2 Hub is mentioned here\nThe USB C port has 24 pins. 12 pins on one side and 12 pins on another. The pins are in exact opposite sequence with each other. To detect the Type C plug orientation, a USB HUB uses type C mux IC. The MUX IC used to set the proper connection of Type C ports SS lines with the USB HUB controller. It used CC pins to detect the plug orientation. VL162 is used a USB Mux in this project. Test Board USB-C Hub used in product ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/usb-hubs/usb3.2-type-c-hub/","summary":"Description USB-C is a 24-pin USB connector system with a rotationally symmetrical connector. Its symmetrical design means it can be inserted either way up or down. The USB C port can be used for video adaptors Alternate mode. It enables adapters to output video from that same USB-C port to HDMI, DisplayPort, VGA, and other types of video connectors. It also used for USB Power Delivery (USB PD) to charger/power the devices.","title":"USB-C Port USB3.2 HUB"},{"content":"\rDescription USB3.2 is a SuperSpeed USB protocol that can achieve maximum bandwidth of 10Gbits/s.\nVLabs VL822 IC is used as a USB3.2 hub controller. VL822 features 1x upstream port and 4x/2x downstream ports, all of which support 10Gbps USB 3.1 Gen 2 operation. The downstream ports can support any combination of SuperSpeed Plus (10Gbps), SuperSpeed (5Gbps), High Speed (480Mbps), Full Speed (12Mbps), and Low Speed (1.5Mbps) devices. VL822’s integrated USB 2.0 hub features Multiple Transaction Translators, providing increased bandwidth and performance when multiple Full Speed devices are simultaneously used.\nThe USB 3.2 is a 10 core cable.\nTwo cores are D+/D- for USB2.0 data Two cores are SSTX+/SSTX- for USB3.2 transmitter data Two cores are SSRX+/SSRX- for USB3.2 receiver data Two cores are VCC \u0026amp; GND for the power Two cores are shield over the SSTX and SSRX lines The USB Hub system consist of below blocks\nUpstream Port (connection with the host PC) Downstream port (connection with the peripherals) Main USB HUB controller (manages the communication between peripheral devices and the computer system) Overcurrent protection IC (Protect the downstream port from short circuit and overcurrnet). ESD protection for downstream and upstream port (To protect the circuit from ESD) Flash memory for the Hub controller program. A special firmware is provided by the Vlab for enabling the VL822 superspeed mode. The firmware can be burned using Vlab Windows utility.\nPCB Layout A PCB layout has to follow high speed PCB design guideline provided by Vlab while designing the PCB to maintain signal integrity and avoid EMC issues.\nPCB Details\n6 Layer Impedance controlled PCB. SuperSpeed trace impedance: 85 Ohm. D+/D- trace impedance : 90 Ohm Differntail pair length matching USB Hub used on PCB ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/usb-hubs/usb3.2-hub/","summary":"Description USB3.2 is a SuperSpeed USB protocol that can achieve maximum bandwidth of 10Gbits/s.\nVLabs VL822 IC is used as a USB3.2 hub controller. VL822 features 1x upstream port and 4x/2x downstream ports, all of which support 10Gbps USB 3.1 Gen 2 operation. The downstream ports can support any combination of SuperSpeed Plus (10Gbps), SuperSpeed (5Gbps), High Speed (480Mbps), Full Speed (12Mbps), and Low Speed (1.5Mbps) devices. VL822’s integrated USB 2.","title":"USB3.2 Gen 2 HUB"},{"content":" Client SEIDONICS Product Description This is 10 channel data logger that monitor and log the voltage, current and temprature readings. The datalogger logs stores the data into the onboard memory card. Datalogger is connected with a web server using GSM internet service. All the log data is synced on the cloud server that can be viewd anywhere from the world. It is compatible with any 2G GSM network provider. The datalogger has 10 individual voltage AND CURRENT channels. All channel voltage can be scanned in 3 seconds. The memory card is directly mountable (FAT32) in Windows so user can check the logs file easily using Windows machine. The log files contains the timestamp, voltage reading, currect reading, temperature reading in a comma separate text file. The product has a USB port, User can access the logs using a customized windows utility. The utility is also used to set the logging interval,watch the live current and volatge readings, set the date/time nd to check other network parameters.\nWorking The device has MSTB connector to connect the batteries +ve/-ve terminal. For each battry, ther is provison for 1 current transformer(CT).A hall effect current transfer (CT) is used to measure the DC current flowing through the battery.The CT has to be inserted into the negative cable of the battery. When an input current flows, a magnetic field proportionate to the input current is generated in the core gap, and the magnetic field is converted to a voltage by the hall IC to obtain a value of an output voltage proportionate to the input current. The analog voltage is converted into digital value and digital values are mapped to the current scale. The hall effect CT also measure the current flow direction,that used to detect the charging and discharging cycle of the battery. Similar analog to digital method is used to read the battery voltage.\nThe device measure the voltage,current and tempartur paramerter after fix scannning interbval. minimum scanning time cab be set to 3 sec. After each scan it send the data to the cloud server using GPRS communication. Server runs a database in which all log entries are stoed. If there is GSM connection problem then device store all the log data into the memory card and once the device connects with server, it transfers all the locall stored data to the database.\nProduct Specifications 10 Voltage channel. 10 Current channel (Hall effect CT). Temperature measurement. Voltage measurement range: 0V - 20V DC (Protection upto 80V) Current measurement range: 0V - 50A DC Voltage measuring accuracy: +/-50mV GSM Connectivity (2G). Cloud Server SD Card slot for storage. Max 16Gb supported. Scanning Time: 3 Sec LED indications for power, logging. Onboard RTC (Real Time Clock). Overvolage detection USB Connectivity Hardware Specification ESP32 Controller 24 bit ADC Product Photos/Videos ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/10-ch-iot-datalogger/","summary":"Client SEIDONICS Product Description This is 10 channel data logger that monitor and log the voltage, current and temprature readings. The datalogger logs stores the data into the onboard memory card. Datalogger is connected with a web server using GSM internet service. All the log data is synced on the cloud server that can be viewd anywhere from the world. It is compatible with any 2G GSM network provider. The datalogger has 10 individual voltage AND CURRENT channels.","title":"10 Channel Voltage and Current IoT Data Logger"},{"content":" Client SEIDONICS Product Description This is single channel data logger developed to monitor the battery voltage deployed to power surveillance systems at big warehouses. This device continuously monitors battery voltage and gives alert in case of battery failure. It has a LCD display that shows the live battery parameters like voltage, current and temperature. Device stores all the recorded data into a memory card. The memory card is directly mountable (FAT32) in Windows so user can check the logs file easily. The log files contains the timestamp, voltage reading, currect reading, temoarature reading in a comma seprated text file. The product has a USB port, User can access the logs using a customized windows utility. The utility is also used to set the logging interval,watch the live current and volatge readings, set the date/time.\nWorking The battery +ve/-ve terminal needs to be connected to the BAT port of the datalogger. A hall effect current transfer (CT) is used to measure the DC current flowing through the battery.The CT has to be inserted into the negative cable of the battery. When an input current flows, a magnetic field proportionate to the input current is generated in the core gap, and the magnetic field is converted to a voltage by the hall IC to obtain a value of an output voltage proportionate to the input current. The analog voltage is converted into digital value and digital values are mapped to the current scale. The hall effect CT also measure the current flow direction,that used to detect the charging and discharging cycle of the battery. Similar analog to digital method is used to read the battery voltage.\nThe product does not need the external power supply for the working. It runs on the battery power and consume very little current so it wont drain the battery significantly. External power supply port is also provided if required. A wakeup button is used to turn ON the display and shows the voltage and current readings. After some time display gets turn OFF to save the battery power.\nProduct Specifications 1 Voltage channel. 1 Current channel (Hall effect CT). Temperature measurement. Voltage measurement range: 0V - 20V DC Current measurement range: 0V - 30A DC Voltage measuring accuracy: +/-50mV SD Card slot for storage. Max 16Gb supported. LCD Alphanumeric Display. 9V optional power input. LED indications for power, logging. Onboard RTC (Real Time Clock). USB Connectivity Interactive Windows software for live voltage display, Displaying stored records. Product Photos/Videos Windows Utility (Software) for downloading the logs\r","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/single-channel-voltage-current-datalogger/","summary":"Client SEIDONICS Product Description This is single channel data logger developed to monitor the battery voltage deployed to power surveillance systems at big warehouses. This device continuously monitors battery voltage and gives alert in case of battery failure. It has a LCD display that shows the live battery parameters like voltage, current and temperature. Device stores all the recorded data into a memory card. The memory card is directly mountable (FAT32) in Windows so user can check the logs file easily.","title":"Single Channel Voltage and Current Data Logger"},{"content":" USB5744 EVM\nDescription USB3.0 is a SuperSpeed USB protocol that can achieve maximum bandwidth of 5Gbits/s.\nMicrochip USB5744 IC is used as a USB3.0 hub controller. Microchip’s USB5744 IC is a 4 port, SuperSpeed (SS)/Hi-Speed (HS), low power, low pin count configurable and fully compliant with the USB 3.1 Gen 1 specification. The USB5744 also supports Full Speed (FS) and Low Speed (LS) USB signaling, offering complete coverage of all defined USB operating speeds.\nThe USB 3.0 is a 10 core cable.\nTwo cores are D+/D- for USB2.0 data Two cores are SSTX+/SSTX- for USB3.2 transmitter data Two cores are SSRX+/SSRX- for USB3.2 receiver data Two cores are VCC \u0026amp; GND for the power Two cores are shield over the SSTX and SSRX lines The USB Hub system consist of below blocks\nUpstream Port (connection with the host PC) Downstream port (connection with the peripherals) Main USB HUB controller (manages the communication between peripheral devices and the computer system) Overcurrent protection IC (Protect the downstream port from short circuit and overcurrnet). ESD protection for downstream and upstream port (To protect the circuit from ESD) The USB5744 is a configurable IC. The superspeed TX+/Tx- lines can be swapped to simplify the routing. The vendor string can be changed using the Microchop MPLAB Windows utility.\nPCB Layout A PCB layout has to follow high speed PCB design guideline provided by Microchip while designing the PCB to maintain signal integrity and avoid EMC issues.\nPCB Details\n6 Layer Impedance controlled PCB. SuperSpeed trace impedance: 85 Ohm. D+/D- trace impedance : 90 Ohm Differntail pair length matching USB Hub used on PCB ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/usb-hubs/usb3.1-hub/","summary":"USB5744 EVM\nDescription USB3.0 is a SuperSpeed USB protocol that can achieve maximum bandwidth of 5Gbits/s.\nMicrochip USB5744 IC is used as a USB3.0 hub controller. Microchip’s USB5744 IC is a 4 port, SuperSpeed (SS)/Hi-Speed (HS), low power, low pin count configurable and fully compliant with the USB 3.1 Gen 1 specification. The USB5744 also supports Full Speed (FS) and Low Speed (LS) USB signaling, offering complete coverage of all defined USB operating speeds.","title":"USB3.1 Gen 1 HUB"},{"content":"Product Description This device continuously detect purity of oxygen machines by using electrochemical sensor and send data to server. Simultaneously device detects patients SPO2 and Pulse rate and send data to server. Whenever device observed any unpredictable values for both it sends alters on patient’s mobile via SMS and Calls.\nDevice ia having unique display to show all values to user live. (16x2) lcd.\nDevice required- internet connectivity and standby bluetooth connectivity in the failuer of internet connectivity. As well as Special android / IOS app developed to view and observe live data\nFeatures Continuously Detection of purity of oxygen machine and send alters. Detection of SPO2 and Pulse rate of patient and send alerts for set limits. Helps to avoid critical situations of patient desaturation. Main O2 Sensor used O2 sensor\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/o2-analyzer/","summary":"Product Description This device continuously detect purity of oxygen machines by using electrochemical sensor and send data to server. Simultaneously device detects patients SPO2 and Pulse rate and send data to server. Whenever device observed any unpredictable values for both it sends alters on patient’s mobile via SMS and Calls.\nDevice ia having unique display to show all values to user live. (16x2) lcd.\nDevice required- internet connectivity and standby bluetooth connectivity in the failuer of internet connectivity.","title":"IOT Based Oxygen and SPO2 Analyzer"},{"content":" Client SEIDONICS Product Description The Product is used in the \u0026lt;strong\u0026gt;EXIDE\u0026lt;/strong\u0026gt; battery manufacturing plant to measure and log the voltages of the Lead Acid batteries while its manufacturing process. The product panel is divided into 48 datalogging cards and each card can measure and log the voltages of 20 lead acid batteries connected in series. So each panel can log the data of 960 batteries. The recorded data is stored in the card and All cards data stored in the MySQL database (Windows Machine). Each card is connected to a 24 channel Ethernet switch and the ethernet switch is connected to the local computer for the connectivity.\nProduct Architecture Working In the battery manufacturing process the battery is charged for 48 hours by a rectifier and after particular interval the battery voltages needs to be measure to check if the batteries are charging accordingly to the charge cycle.the Card has two trigger inputs which is connected to the rectifier. once the trigger comes from the rectifier its log the data of that time. Windows machine runs a MySQL database server to store all the logs and trigger data from the card and Custom designed software is installed to read/display all the logged voltages.\nOnce the windows server comes online the card transfers all the locally stored data (JSON) into the MySQL database by executing the SQL quires. The software is also used to add a new card, set the trigger interval,create a batch of batteries. Each card is identified by its IP address.\nThe card has a overvolatge detection feature that detects if any battery voltages goes above the the threshold voltage limit. If the overvoltage condition occures then the buzzer and Alert indicator on the card turn ON.\nProduct Specifications 20 Voltage measurement channels. Voltage measurement range: 0V - 48V DC Voltage measuring accuracy: +/-50mV 2 Trigger Channels. TCP/IP connection with server (over Ethernet) 4Gb onboard storage memory. 12V Card Input voltage LED indications for card power, Server live, logging, Alert Onboard RTC (Real Time Clock). Battery Overvoltage detection. (Each channel is ) Interactive Windows software for live voltage diaplay, Displaying stored records and Report Generation. Single Server/Software supports upto 7 Panels. (Total 6720 battries) Product Photos/Videos ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/single-card-20ch-datalogger/","summary":"Client SEIDONICS Product Description The Product is used in the \u0026lt;strong\u0026gt;EXIDE\u0026lt;/strong\u0026gt; battery manufacturing plant to measure and log the voltages of the Lead Acid batteries while its manufacturing process. The product panel is divided into 48 datalogging cards and each card can measure and log the voltages of 20 lead acid batteries connected in series. So each panel can log the data of 960 batteries. The recorded data is stored in the card and All cards data stored in the MySQL database (Windows Machine).","title":"20 Channel voltage Datalogger (Single Card)"},{"content":" Client SLIGHTGEN Product Description i-Cooler controller converts the manual cooler into remote controlled cooler with multiple features. The standard cooler has a know that used to ON/OFF cooler and to set the fan speed. Installing this device into the cooler gives the functionality like an AC. This device can be installed into any standard cooler available in the market with very easy connections.\nThe cooler controller comes with dry run protection features that turn OFF the water motor once the water gets drain. It protects the water motor from burning due to dry run and increases the motor life.\nAnother features is i-mode which maintain the humidity of room by turning ON/OFF the water motor periodically. This feature saves the water and electricity. Sometimes due to continuously run of water motor the rooms gets humid so this mode helps to reduce the humidity problem.\nThe cooler controller comes with sleep timer features that will turn off the cooler after the predefined set time. This will prevent the room to get over cooled due to continuos running of the cooler. This saves the electricity and water as well.\nAlong with all these features the i-cooler controller can set the fan speed and turn ON/OFF the cooler. All these functions can be set by the wireless remote and all the parameter like i-mode ON time, OFF time, Sleep Time can be set as per user need.\nThe cooler controller comes with three variant.\nRolling Display Model Single Display Model Single Display with Knob model Product Features Remote Control Turn your cooler/water pump ON/OFF remotely from any corner of the room just at your finger tips. Fan Speed Control No need to get up to control fan speed of your cooler. With three speed (Low/Medium/High) options on the remote, speed up or down your cooler fan remotely. Dry Run Protection With this feature, you need not worry about your pump running even when cooler runs out of water. iCooler Controller senses pump’s dry run and turns the pump off immediately saving power and pump burning. Alpha Numeric display and LED indicators Get visual information about speed level, timer status, low water level indication, etc Sleep Timer Set your device on timer and you need not worry about turning it off. Once the time is up, cooler will turn off itself. i-mode This mode saves water and electricity by consuming less of both and also helps to maintain the humidity of the room. This has become possible with our intelligent PUMP-ON principle. Pump turns on only for the period that sides need to wet and then turns off. Universal compatibility It is compatible with all cooler models. Working The cooler controller has a remote which runs on a infrared technology. The cooler controller has an IR receiver which decodes the data sent by the remote. A micontroller is used to decode the IR data, control the 7-segment LED display, control the relay for fan speed ,control the water motor and sense the water level. A water level sensor is dipped into the water tank of cooler. once the water level goes below to the sensor limit it will turn oFF the water motor and shows the Low water indications.\nThe installation of device is very simple. Below is the wiring diagram to connect the device with the standard cooler. i-Cooler Remote Control Technical Specifications Supply Voltage: 230V AC Maximum Motor Current Supported: 7 Amp Maximum Pump Power Supported: 100W Maximum Motor Power Supported: 1000W Remote Range: 25 feet Fan Speed Steps: 3 Speed Product Photos Product Demo Videos i-Cooler Controller with rolling display (Complete Product Demo) i-Cooler Controller with Single display i-Cooler Controller with manual knob i-Cooler Controller Prototype installed on Cooler ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/i-cooler-controller/","summary":"Client SLIGHTGEN Product Description i-Cooler controller converts the manual cooler into remote controlled cooler with multiple features. The standard cooler has a know that used to ON/OFF cooler and to set the fan speed. Installing this device into the cooler gives the functionality like an AC. This device can be installed into any standard cooler available in the market with very easy connections.\nThe cooler controller comes with dry run protection features that turn OFF the water motor once the water gets drain.","title":"i-Cooler Controller"},{"content":" Client Chargemile Technologies Product Demo Video Product Description Chargie is a Smart EV charging station which will be installed on the parking area. User can book the charging station and slot from the mobile app. The user will get charges depending on the electricity used for charging.\nThe Chargie comes with two variant.\nSingle Charging Point Used in the footpath / In small space Multiple Charging Point Used in the big parking space. Charge Multiple Vehicles at a time. Working The Chargie device is connected to a cloud server using the GSM network. The device has a 16Amp power socket which can be used for plugging any bike/car charger. Once the user starts the charging from the app, device measures all the parameters like voltage, current and wattage and calculate the total energy consumed in the charging cycle. Depending on the unit consumed user gets billed. The user can put the vehicle charger inside the Chargie so that it will be safe while charging.\nProduct Specifications 2G GSM Network Max Output Current: 16Amp Overvoltage and Overcurrent Protection Energy Metering APP \u0026amp; Hardware Demo ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/ev-charging-station/","summary":"Client Chargemile Technologies Product Demo Video Product Description Chargie is a Smart EV charging station which will be installed on the parking area. User can book the charging station and slot from the mobile app. The user will get charges depending on the electricity used for charging.\nThe Chargie comes with two variant.\nSingle Charging Point Used in the footpath / In small space Multiple Charging Point Used in the big parking space.","title":"EV Charging HUB (Chargie)"},{"content":" Client Spring Technologies Product Description The Nulock is a IoT based smart lock which is designed to used on a petrol tanker vehicle. The lock only opens in the area where the tanks refill or empty. It has a geo-fencing feature so lock only opes in the specific area which is set by the admin. It helps to illegally stealing petrol from the tanker when its traveling. The product is design to comply intrinsic safety standards given by the Petroleum \u0026amp; Explosives Safety Organization (PESO).\nThe Nulock has two variants\nKey and Lock System Fixed lock System Working of Key Lock System This system uses three devices; Mobile app, key and lock. An Andriod app is used to create the task (Access pass) for opening or locking the lock. The task contains the parameters like date \u0026amp; time slot for opening the lock, lock id, access code, area code etc. The admin create the task when tank comes in the refilling area(Depo). It uses mobile GPS signal to detect if the tanker is in the refilling area. Parallelly, the admin creates a task for opening the lock in unloading area(In the petrol pump station). An OTP is generated for the unloading task.\nAfter creating the task,app checks all the parameters and load the task into the key using the bluetooth connectivity. Once the key is loaded with the task then the user carry the key in the hazardous zone (refilling area). It is not permissable to carry the mobile phone in the hazardous zone because of the chances of the explosion due to cell phone signals.\nThe key is designed/certified to work in a hazardous environment. It has all the protection features to prevent any spark/ignition in the key/lock that can cause an explosion. The key has a small lead acid battery which powers the key and lock. No battery is used in the lock. Each lock has its unique lock id . When the user insert the preloaded task key in the lock then it communicates with the lock\u0026rsquo;s controller and verify if its the correct lock. After successful verification it opens the lock. The same key can be used to open multiple lock with different lock id.\nWhen the tanker reach at the petrol pump then the user enter the OTP generated by the admin for opening the lock. after entering the OTP in the app, The app verify if its a valid OTP, valid location using the GPS co-coordinate and if its a correct date and time slot. After successful verification the task is loaded into the key and the key is used to open the lock.\nA special protocol is implement to transfer power and data using only two wires from the key to lock. Bluetooth BLE mode is used to transfer the data from mobile to key.\nWorking of Fixed lock model This model uses a non breakable lock which is permanently fitted on the tanker. No mobile device or key is used for the locking/unlocking operation. A dedicated main controller is installed in the tanker which is connected to the lock through a wired connection. The main controller device has inbuilt GPS to track the location and GSM for communication with the server. The task creating process is same as the above process. An app is used to create the task which will then synchronize with the main controller via the internet. Admin has to create a task and OTP to open the lock.\nThe controlling device has a keypad and display which used to enter the OTP and check the current status of the process. One controller can be connected to the multiple locks using a RS485 protocol. The locks are installed on the multiple outlet valves of the tanker. The controller and lock are designed to comply the intrinsic safety guideline.\nMain Controller\nLock Installed on Petrol Tanker\nDemo of Key Lock Model Demo of Fixed Lock Model ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/smart-lock/","summary":"Client Spring Technologies Product Description The Nulock is a IoT based smart lock which is designed to used on a petrol tanker vehicle. The lock only opens in the area where the tanks refill or empty. It has a geo-fencing feature so lock only opes in the specific area which is set by the admin. It helps to illegally stealing petrol from the tanker when its traveling. The product is design to comply intrinsic safety standards given by the Petroleum \u0026amp; Explosives Safety Organization (PESO).","title":"IoT based Smart Lock (NuLock)"},{"content":" Client Air Cycle System Product Description The Penguin-Air™ is the world’s lightest industrial-grade, air-powered respirator system. It filters out 99.7% of particulates using patent-pending technology and provides cool relief with double filtered air to the mask in real-time, providing the comfort you need all day on a single charge. Now you can stay safe and stay open. (Not a medical device at this time)\nProduct Website Aircyclesystem.com\nProduct Photos Product Video ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/airmask/","summary":" Client Air Cycle System Product Description The Penguin-Air™ is the world’s lightest industrial-grade, air-powered respirator system. It filters out 99.7% of particulates using patent-pending technology and provides cool relief with double filtered air to the mask in real-time, providing the comfort you need all day on a single charge. Now you can stay safe and stay open. (Not a medical device at this time)\nProduct Website Aircyclesystem.com\nProduct Photos Product Video ","title":"Peguin-Air 99+"},{"content":" Designed By CEMTREX Product Description Working Techincal Specifications ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/usb-pd/","summary":" Designed By CEMTREX Product Description Working Techincal Specifications ","title":"90W USB Type C PD Charger"},{"content":" Designed By SLIGHTGEN Product Description ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/college-projects/accident-alert/","summary":" Designed By SLIGHTGEN Product Description ","title":"Accident Alert system"},{"content":"\r","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/alexa-bell/","summary":"\r","title":"Alexa bell"},{"content":"","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/audio-amplifer/","summary":"","title":"Audio Amplifier"},{"content":" Designed By CEMTREX Product Description Working Techincal Specifications ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/door-access-control/","summary":" Designed By CEMTREX Product Description Working Techincal Specifications ","title":"Biometric Door Access System"},{"content":" Designed By SLIGHTGEN Product Description ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/college-projects/biometric-locker/","summary":" Designed By SLIGHTGEN Product Description ","title":"Biometric Locker"},{"content":" Designed For CEMTREX SMARTDESK Product Description BlueBox is designed to be used in the SmartDesk product. It is designed to connect the mobile phones and any wireless bluetooth earphone over the bluetooth to the SmartDesk and route the analog audio signal and mic signal to different audio output devices. It gives the features like calling, phonebook reading, SMS reading from the Smartdesk Bluebox Utiltiy. The Mic and Speaker analog audio signals can be routed to smartdesk speakers/bluetooth earphone/mobile phone. It works as center hub for swithcing all the analog audio signals.\nWorking Bluebox uses two audio bluetooth modules. One bluetooth module is used to connect with the mobile phone to control the features like calling, music control, phonebook and SMS checking. Another bluetooth module is used to connect with the bluetooth earphone. The module uses the below bluetooth audio profiles for the commnication with the mobile phone and earphone.\nAdvanced Audio Distribution Profile (A2DP) To stream the mobile phone audio to the speakers or bluetooth earphone connected to the bluebox. Audio/Video Remote Control Profile (AVRCP) To use the play/pause/stop commands of music player from bluebox utility. Hands-Free Profile (HFP) To use the desktop\u0026rsquo;s mic/ bluetooth earphone mic for answering the call. Headset Profile (HSP) To ring, answer, a call, hang up and adjust the volume of mobile phone. Message Access Profile (MAP) To download the SMS from mobile phone to bluebox windows utility. Phone Book Access Profile (PBAP, PBA) To access the phonebook from the mobile phone and get the missed call, incoming calls and outgoing call history from mobile phone. The bluebox has Analog audio mux ICs to route all the audio data coming from bluetooth modules, desktop audio In/Out port and bluetooth earphone. All this controlling is done by a micronctoller. The microcontroller controlles the bluetooth module audio profiles using the UART commands and analog MUX controlled by a GPIO. The bluebox is connected to Smartdesk CPU over the USB cable for communication with the Windows bluebox utility using the USB to TTL bridge IC.\nA Windows utility is designed to control all the bluebox features like pairing the mobile/earphone,getting device status, dial/answer the calls,downloading phonebook,getting call history and reading SMS. PyQt toolkit is used to design the UI.\nBlueBox Features Bluebox can be connect upto two mobile devices smilutenously. Answer and make call using Smartdesk speakers or bluetooth earpods Stream audio from SmartDesk PC/ Mobile phone to earpods directly. Receive/End call from Earpod button Auto disconnection of PC audio stream during incomg/outgoing calls. Switch call audio stream between earpods and Smartdesk speakers. Download incoming, missed, outgoing call history of both mobiles. Uses clear voice capture technology (noise cancellation) for calling. Download phonebook of both mobiles. Read incoming SMS. Mobiles automatically connect to BlueBox when they are in range. PC Audio stream automatically switch to SmartPods when they get connected (its default setting. It can be changed if required). We can also control the switching manually using GUI. Mobile audio (songs) can be streamed to the SmartDesk speaker. In this case, PC audio stream will be disconnected. Easy pairing of any Bluetooth Earphone/SmartPods and any latest Smartphone to the smartdesk. Provision for getting notifications like; mobile connection, SmartPod connection, incoming call, etc. Main Components Bluetooth 4.0 Module BC127 Microcontroller (Atmega644P) USB to TTL IC (CP2102) Quad SPDT Audio Switch MAX4740 BlueBox Windows Utility BlueBox functionality in depth Video ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/bluebox/","summary":"Designed For CEMTREX SMARTDESK Product Description BlueBox is designed to be used in the SmartDesk product. It is designed to connect the mobile phones and any wireless bluetooth earphone over the bluetooth to the SmartDesk and route the analog audio signal and mic signal to different audio output devices. It gives the features like calling, phonebook reading, SMS reading from the Smartdesk Bluebox Utiltiy. The Mic and Speaker analog audio signals can be routed to smartdesk speakers/bluetooth earphone/mobile phone.","title":"BlueBox"},{"content":" Designed By CEMTREX Product Description cover the USB to I2S DAC part as well\nWorking Techincal Specifications single channel daddw\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/bluetooth-car/","summary":"Designed By CEMTREX Product Description cover the USB to I2S DAC part as well\nWorking Techincal Specifications single channel daddw","title":"Bluetooth car"},{"content":" Designed By CEMTREX Product Description Working Techincal Specifications ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/bluetooth-speaker/","summary":" Designed By CEMTREX Product Description Working Techincal Specifications ","title":"Bluetooth Speaker"},{"content":" Designed By CEMTREX Product Description cover the USB to I2S DAC part as well\nWorking Techincal Specifications single channel daddw\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/designing-power-converters/","summary":"Designed By CEMTREX Product Description cover the USB to I2S DAC part as well\nWorking Techincal Specifications single channel daddw","title":"Desinging Power Converters"},{"content":" Designed By SLIGHTGEN Product Description ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/college-projects/dtmf-controller-robot/","summary":" Designed By SLIGHTGEN Product Description ","title":"DTMF controlled robot"},{"content":"","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/educational-kit/","summary":"","title":"Educational Demonstration Kit"},{"content":" single channel PROJECTS ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/educational-projects/","summary":" single channel PROJECTS ","title":"Educational projects"},{"content":"https://en.wikipedia.org/wiki/SAE_J1772\nDescription Many Electric Vechical (EV) useS SAE J1772 Type 1 Connector for the charging. The Type 1 connector is connected to a \u0026ldquo;electric vehicle supply equipment\u0026rdquo;(EVSE). EVSE supply AC power to the vehicle\u0026rsquo;s on-board charger, which then converts it to the direct current (DC) needed to recharge the battery.\nSAE J1772 / IEC 62196-2-1 Type 1 Connector. Different types of connector are used in differnet countries. Below is the figure of EV charger connector standars There are two type of EVSE (Charger)\nAC Charger It only supply AC single phase/3 phase power to the vehicle. Rectifier is in the card which convertes the AC into DC and charges the battries. DC Charger The Charger itself rectifies the AC into DC. DC supply is fed to the vehicle. There are two EV side connectors, type-1 and type-2.\nType-1 connector (J1772): It consists of 5 pins. It has charging voltage upto 250V and charging current upto 32 A. Hence AC charging power upto 7 KW can be possible. Type-2 connector (62196-2): It supports single phase charging and three phase charging with charging voltage upto 500V and charging current upto 63A. Three phase 400 V charging at 32 A represents charging power of 22 KW. In this system, AC charger is impliment with IEC 62196-2-1 connector.\nThe IEC 62196-2-1 has 7 pins. Position Signal Description 1 L1, L2, L3 AC Phase L1 for Single Phase supply L1, L2, L3 for three Phase supply 2 N AC Neutral 3 PE Protective Earth / Ground 4 PP \u0026ldquo;Proximity Pilot\u0026rdquo;/\u0026ldquo;plug present\u0026rdquo;, which provides a signal to the vehicle\u0026rsquo;s control system so it can prevent movement while connected to the EVSE (Charging station), and signals the latch release button to the vehicle. 5 CP \u0026ldquo;Control Pilot\u0026rdquo; is a communication line used to signal charging level between the car and the EVSE. 1 KHz square wave at +/-12V is generated by EVSE on the pilot pin to detect the presence of the vehicle, communicate the maximum allowable charging current and control the charging begin/end. Working The signaling protocol has been designed for the negotation with the vehicle. A microcontroller is used in the EVSE to impliment the protocol. The protocl has the diffenr charging states thats need to follow to start the charging. The charging states checks if the vehicle is connected properly, set the charging current, provide the safet feutre. CP pin is used for the communication.\nThe hardware includes a controller (ESP32), Relay to turn ON/OFF the charger supply, Opamp and other components.\nSignaling Circuit Control Pilot (Mode) The controller uses a 1 kHz square wave at ±12V on the control pilot that is connected back to the protective earth (GND) on the vehicle side by means of a resistor and a diode (voltage range ±12.0±0.4 V). It forms a voltage divider network. The controller measure the analog voltage present on the CP pin to check the state of the charger(explained below).\nCharging Sequence [State A] When the charger is not connected, The controller puts +12V on the control pilot wire. till the votage stays at 12V controllers keep the charger OFF.\nWhen the plug has been connected, the vehicle places a 2.74kΩ load on the pilot line, It forms a voltage divder circuit (1kΩ in the EVSE and 2.74kΩ in the vehicle) which drops the voltage to 9 V. It indicate that EV is ready for charging. [State B] When the voltage is set to 9V, Controller enables the PWM, which signals the vehicle how much current it can draw. The EVSE also closes the relays, providing power to the vehicle.\n[State C] The vehicle starts to draw power and switches to the 822Ω load, which drops the voltage to 6 V, signaling the EVSE that charging has started. Most vehicles continue to pull low amounts of power in state C, even when fully charged, so the charging process is ended by unplugging the cable, which returns the voltage to 12 V. The controller measures this process and closes the relays and returns to State A.\nThe diode preset in the vehicle will only make for a voltage drop in the positive range; any negative voltage on the CP-PE loop is blocked by D1 in the vehicle. It a safety function. Like if the connector drops in the water or any short circuit happens on the pin then negative voltage also transfers on the CP pin. So if that happens the controller turn OFF the power.\nControl Pilot (Current Control) The duty cycle of the pilot signals communicates the limit of current the EVSE is capable of supplying to the vehicle; the vehicle can then use up to that amount of current for its charging circuitry. This current rating is primarily determined by the electromechanical components in the EVSE, such as conductors, relays, contactors, and the service connection.\nThe controller uses the PWM to set the 1Khz wave duty cycle as per the current provided by charger.\nExample Example of CP line communication Proximity Pilot (PP) PP pins uses to turn off the power in case of the charger removal. If the connector removed directly when the chrging is going on, it can create a spark due to high current flowing through the connector. It also uses to indicate the cable current carring capacity.\nThe proximity pin (PP) is connected to the switch (S3). Switch is mechanically linked to the connector latch release actuator. During charging, the EVSE side connects the PP-PE loop via S3 and a 150 Ω R6; when opening the release actuator a 330 Ω R7 is added in the PP-PE loop on the EVSE side which gives a voltage shift on the line to allow the electric vehicle to initiate a controlled shut off prior to actual disconnection of the charge power pins.\nEVSE Product Hardware CP block schematic used in Product EVSE Product PCB Photo Reference https://en.wikipedia.org/wiki/SAE_J1772 https://openev.freshdesk.com/support/solutions/articles/6000052074-basics-of-sae-j1772 https://openev.freshdesk.com/helpdesk/attachments/6004101695 https://www.ti.com/lit/ug/tidub87/tidub87.pdf?ts=1680494703780\u0026amp;ref_url=https%253A%252F%252Fwww.google.com%252F ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/ev-ch-hub-uk/","summary":"https://en.wikipedia.org/wiki/SAE_J1772\nDescription Many Electric Vechical (EV) useS SAE J1772 Type 1 Connector for the charging. The Type 1 connector is connected to a \u0026ldquo;electric vehicle supply equipment\u0026rdquo;(EVSE). EVSE supply AC power to the vehicle\u0026rsquo;s on-board charger, which then converts it to the direct current (DC) needed to recharge the battery.\nSAE J1772 / IEC 62196-2-1 Type 1 Connector. Different types of connector are used in differnet countries. Below is the figure of EV charger connector standars There are two type of EVSE (Charger)","title":"EVSE Type 2 AC Charger"},{"content":" Designed By SLIGHTGEN Product Description ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/college-projects/green_house/","summary":" Designed By SLIGHTGEN Product Description ","title":"Green House Monitoring"},{"content":" Project Description Haptic Robotic Arm is designed to control the robotic arm by recognizing the hand gestures. The hand gestures are nothing but, hand movements. Gesture based control is more popular in the field of robotics, which interfaces human movement in accordance to the machine / device. Haptic technology is used for designing a robotic arm. This Haptic technology is defined “to make use of the feedback from the sense of touch that enabled to control devices remotely”.\nWorking A developed haptic robotic arm is operated by detecting the motion of hand using the potentiometer. The designed robotic arm mimics the actions of the hand and it controls the robotic arm. When the hand moves it changes the potentiometer resistance value. The resistance value is converted into analog voltage and its measured by the microcontroller. microcontroller converts tha analog voltage into digital voltage (ADC). The controller then control the motion of motor according to the resistance of potentiometer.\nThe system can be made more accurate and responsive using the flex sensor on hand and encoders on the motor for the feedback. Servo motors are the better alternative to the DC geared motor used in the project.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/haptic-hand/","summary":"Project Description Haptic Robotic Arm is designed to control the robotic arm by recognizing the hand gestures. The hand gestures are nothing but, hand movements. Gesture based control is more popular in the field of robotics, which interfaces human movement in accordance to the machine / device. Haptic technology is used for designing a robotic arm. This Haptic technology is defined “to make use of the feedback from the sense of touch that enabled to control devices remotely”.","title":"Haptic Robotic Arm"},{"content":" Designed By SLIGHTGEN Product Description ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/college-projects/home-automation/","summary":" Designed By SLIGHTGEN Product Description ","title":"Home automation"},{"content":" Designed By CEMTREX Product Description cover the USB to I2S DAC part as well\nWorking Techincal Specifications single channel daddw\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/home-automation/","summary":"Designed By CEMTREX Product Description cover the USB to I2S DAC part as well\nWorking Techincal Specifications single channel daddw","title":"Home Automation"},{"content":" Designed By CEMTREX Product Description cover the USB to I2S DAC part as well\nWorking Techincal Specifications single channel daddw\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/industrial-demo-kit/","summary":"Designed By CEMTREX Product Description cover the USB to I2S DAC part as well\nWorking Techincal Specifications single channel daddw","title":"Industrial Demo Kit"},{"content":" Designed By CEMTREX Product Description Working Techincal Specifications ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/iot_bane/","summary":" Designed By CEMTREX Product Description Working Techincal Specifications ","title":"IoT Bane (SmartDesk)"},{"content":"Product Description This system is designed for Electric Vehicles Charging station. The system is divided into two cards. One Card is a Master card which has the GSM Module, RTC and 2 output channel. This card can measure the energy consumption of two load. Second card is a expansion relay card which has 4 output channel and 6 output channel for solenoid. Solenoids are used to lock the compartment where user can put his charger while charging. Relay card is controlled by a the master card.\nSystem Architecture Working Energy measurement The master card is connected to one step down transformer(PT) to measure the AC line voltage and 6 seperate current transformer (CT) to measure the current flow while charging. The controller uses a Open source library (EmonLib) for calculating the energy consumption. The controller measures different parameters like voltage, current, wattage, power factor. USing these parameter, the controller calculates the total energy consumed while the charging cycle\nFirmware \u0026amp; IoT The ESP32 runs on a FreeRTOS OS. FreeRTOS can run multiple threads so that controller can measure the energy consumption of all 6 devices simultaneously. For the communication with the cloud, a Thinger platform is used. It creates a secure channel between the IoT device an cloud server. For the internet connectivity a 2G GSM module is used.\nIf any network issue found between the master card and server then the master card stores all the readings in the flash memory. Once the servers gets live it sends all the data to cloud server. A Thinger APIs are written in the ESP32 for accessing the data and controlling the master card.\nUser can schedule a task for specific channel that will turn ON the power of that channel. E.g. If user schedule a task for the date 12/03/2022 and time slot between 13.00 to 14.00 then the master card turn on the for on that data and time. In that 1 hour time, master card will calculate the energy consumption and that data is sent to cloud server.\nBelow are the other APIs used:\nPing: To check if the IoT device is live. Create Task: To schedule a task for specific date and time Delete Task: To delete a scheduled task Read- To read the registered task on the device Reset- To reset the device Available Task Files- To check Available task file on device Locker- To open Locker solenoid Create_Unit_file-To create unit file Delete Unit- Delete Unit File Read Unit- To read unit file RTC_set- to set RTC time Signal Strength- Sim card network strength The response of this APIs will get into the JSON format. A python script is written to test the APIs.\nThe device can be updated with a new firmware using the OTA (Over the Air) features.\nTechnical Specifications Network: 2G GSM 6 Channel for Charging points 6 Channel Input for CT 1 Channel Input for PT 6 Channel output for Solenoid OTA supported Onboard Flash Memory Onboard RTC Overvoltage and Overcurrent Protection FreeRTOS OS Photos Testing the Circuit Variant 2: 4 Channel device PCB Live Reading on Serial Terminal ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/energy-metering/","summary":"Product Description This system is designed for Electric Vehicles Charging station. The system is divided into two cards. One Card is a Master card which has the GSM Module, RTC and 2 output channel. This card can measure the energy consumption of two load. Second card is a expansion relay card which has 4 output channel and 6 output channel for solenoid. Solenoids are used to lock the compartment where user can put his charger while charging.","title":"IoT based Energy Metering"},{"content":" Designed By CEMTREX Product Description Working Techincal Specifications ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/ocp-ic/","summary":" Designed By CEMTREX Product Description Working Techincal Specifications ","title":"OCP Over current protection"},{"content":" Designed By SLIGHTGEN Product Description ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/college-projects/transformer-protection/","summary":" Designed By SLIGHTGEN Product Description ","title":"Overcurrent protection with GSM"},{"content":"\rDescription Electric motorized legs are used in standing desk to adjust the height of the desk as per the user need. The system is divided into three parts\nMotorized Legs Leg has a DC geared motor and encoder Leg Controller Controller the drives the motor inside the legs It synchonize the legs movemnet It has protection circuit like overload. Leg Controller Remote (Control Panel) Remote is used to control the up/down motion of leg Rempte display shows the current posiotn of legs Objective The objective of this project is to design a custom leg remote controller remote that will communicate with the leg controller. The system should be integrate with any product that uses the legs. For the example, It can be implemented in a system where the touchscreen display used or it can be used to control the leg motion from the PC (Using Software).\nSystem There is no documentation provided by the leg vendor and no information is available on the internet explaining the communication between the leg controller and remote. To checked the communication we have reverse engineered the remote control circuit and decoded the protocol and data used for communication. A protocol analyzer device is used to decode the data.\nThe remote controller send the data using UART protocol. The data is used to display the current position of the legs. Controller also send the error messages like overload, motor/cable fault.\nThe System uses the below components\nAtmega328p Controller 16x2 LCD Display CP2102 USB to TTL converter The firmware is written in embedded C language.\nThe system mimics all the functions of the existing remote controller.\nRemote Functions UP/Down: It is used to control the legs motion Memory: To store the current position of leg into the memory so that it can be used to move the legs into preset position Memory Location: The controller has 3 memory slots to store the position. The remote used to calibrate/reset the legs. Display the current positon and error messages The system mimics all the functions of the existing remote controller.\nTest Device Photo System Video Demo video of existing remote controller ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/motorized-leg-remote-emulator/","summary":"Description Electric motorized legs are used in standing desk to adjust the height of the desk as per the user need. The system is divided into three parts\nMotorized Legs Leg has a DC geared motor and encoder Leg Controller Controller the drives the motor inside the legs It synchonize the legs movemnet It has protection circuit like overload. Leg Controller Remote (Control Panel) Remote is used to control the up/down motion of leg Rempte display shows the current posiotn of legs Objective The objective of this project is to design a custom leg remote controller remote that will communicate with the leg controller.","title":"Remote for Motorized Leg Controller"},{"content":" Designed By SLIGHTGEN Product Description It is a very compact, small and light weight device that works like a mini PC. It can replace desktops and laptops in Conference Room, Audio-visual room, Library, Class rooms, School / College Student dormitory Computer Lab, Cyber Cafe, Exhibition, Hospital, Factories, Government departments, Small Enterprise, Institution Computer training center, Office and Warehouse.\nSetting up computers and laptops can be very expensive on large scale and requires high maintenance cost, consumes more power, demand more space and hard are to carry due to their heavy weight. Whereas, S-com is cheap and affordable, requires very low maintenance, consumes very less power, small in size and highly portable. It runs on an open source operating system. To name a few packages GCC, Java, Python, SQLite, Libre office writer, Impress, Calc; various browsers for surfing; e-mail clients like thunderbird, etc.\nWorking The device has VGA output that used to connect the projector in the classroom. Once the deivce power On, its boots the Linux OS. The OS is simillar to Windows OS. USer can connect a Flash drive to the device USB port and access the data from the flash drive. A sepeate mouse can be connected to the S-COM for easy operaion. The OS comes with pre installed Libre Office that used to open the PowerPoint sildes, MS Word documents etc. The device comes with a wireless remote that can be used to change the slides while presentation. Remote also has frequestly used keyboard keys like ESC, TAB, F5. Other navigation and shortcut keys are also present on the remote. Using the remote we can elimate the use of mouse.\nTechincal Specifications Arm7 Quad Core Processor powered Single Board Computer running at 900MHz 1GB RAM. 4 x USB 2 ports. VGA port to connect displays and projectors . 4W Speaker for audio output. Micro SD port for loading your operating system and storing data. Ethernet Port to quickly connect the S-Com to the Internet. Wi-Fi enabled networking. Stream and watch Hi-definition video output at 1080P. Advanced power management: It can provide up to 1.2 AMP to the USB port – enabling you to connect more power hungry USB devices directly. Benefits Power saving: consumes only about 5w power which is far less then power consumed by laptops and desktops Can be controlled wirelessly using IR remote or android phone. No more buffer face – boots up quickly. Easy file sharing in LAN. Build your own workstation – create and manage your documents and spreadsheets with ease using LibreOffice™ Technical support on phone and email. Product Photos/Videos S-COM Installed in College Classroom.\r","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/s-com/","summary":"Designed By SLIGHTGEN Product Description It is a very compact, small and light weight device that works like a mini PC. It can replace desktops and laptops in Conference Room, Audio-visual room, Library, Class rooms, School / College Student dormitory Computer Lab, Cyber Cafe, Exhibition, Hospital, Factories, Government departments, Small Enterprise, Institution Computer training center, Office and Warehouse.\nSetting up computers and laptops can be very expensive on large scale and requires high maintenance cost, consumes more power, demand more space and hard are to carry due to their heavy weight.","title":"S-COM"},{"content":" Designed By SLIGHTGEN Product Description Switch Timer is provided with a 3 pin socket and a digital display. After inserting the plug in the socket we can set the specific time for which we wish to run the device. Depending on our requirement, the running time of the device can be set from a minute to few hours. After that time the supply will automatically cut off.\nWorking Techincal Specifications ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/switch-timer/","summary":" Designed By SLIGHTGEN Product Description Switch Timer is provided with a 3 pin socket and a digital display. After inserting the plug in the socket we can set the specific time for which we wish to run the device. Depending on our requirement, the running time of the device can be set from a minute to few hours. After that time the supply will automatically cut off.\nWorking Techincal Specifications ","title":"Switch Timer"},{"content":" Designed By SLIGHTGEN Product Description ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/sub_project/test/","summary":" Designed By SLIGHTGEN Product Description ","title":"TESTING DOC"},{"content":" Designed By CEMTREX Product Description cover the USB to I2S DAC part as well\nWorking Techincal Specifications single channel daddw\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/usb-audio-dac/","summary":"Designed By CEMTREX Product Description cover the USB to I2S DAC part as well\nWorking Techincal Specifications single channel daddw","title":"USB Audio DAC"},{"content":"\rDescription Many products requires a onboard storage memory to store different data like drivers, manuals so that user can access it over the USB connection. The system should be access over a USB host computer. It should mount as a mass storage device (like drive).\nTo system main parts\nNAND Flash Memory: NAND flash memory is a type of non-volatile storage technology that does not require power in order to retain data. The NAND flash memory is of two types,\nNAND Flash (RAW Memory)\nIt is a flash memory without any controller.\nIt can be interface with a microcontroller by writing a driver and firmware for flash operations.\nIt can be interfaced with NAND flash controller chip (Mentioned in point 2).\nThis types of memory are used in the RAM,SSD.\nManaged NAND\nIt is a memory with NAND controller.\nExample is eMMC/Memory Card, EEPROM IC.\nMemory Controller : The NAND Flash controller chip that is dedicatedly designed to control the operations on Raw Flash Memory and Directly Accessible from Windows as a mass storage (Under HID Devices, No need of Driver development). A flash memory controller is the part of solid-state flash memory that communicates with the host device and manages the flash file system directory. The controller is also responsible for wear leveling, error correction and garbage collection.\nFor the detailed information about the memory interfacing/types, click here.\nSystem Design OTi2272 chip is used as a flash memory controller. It is a high performance USB 2.0 Mass Storage Class Peripheral controller targeted for applications of USB Flash Disks. The design complies with USB High Speed Specification (480Mb/s).\nThe controller Integrates the USB 2.0 transceiver with dynamic feedback control, stable slew rate, independent of external loading for reducing EMI. A Phase Lock Loop (PLL) is embedded provides all clocks needed in this controller.\nThe OTi-2272 accommodates up to 8 Flash devices and works with comprehensive Flash memory technologies available on the market. The controller also has write- protect capability to prevent writing data to Flash.\nThe controlled can be interfaced with different brands flash memory like Hynix, Sandisk, Samsung. Each memory controller has its own list of supported NAND flash memory depending on the flash type MLC/TLC and its density. This list is provided by the flash memory controller vendor. Below is the list example. In this system, A 2GB (8Gbit) NAND flash memory (Hynix H27UAG8T2ATR) is interfaced with the OTI2272 controller.\nThe memory can be mounted in two mode.\nMass Storage: In this mode, the memory mount as a removable flash drive. Like a normal flash drive. In Read/write mode. CD Drive : In this mode, the memory mount in readonly mode like the CD. User can not delete the data from the drive. The memory is configured using the Windows utility provided by Oti. Customized Firmware Vendor provided another utiltiy to lock/unlock the flash drive in mass storage mode. When the drive is unlocked it goes into writable mode. User can copy the files in the drive. This mode is used to copy the files like driver at the time of assembly the product. Once the files are copied, the drive is set into lock mode and user can not format drive or delete the data.\nUtility Uses The utilty is used for the below purpose\nChange the VID/PID (If required) Change the Product and Vendor String Product String is the name that will be displayed in the Windows device manager. Fix the flash memory usable capacity. It use to set the memory size less than its actual capacity. Below is the screenshot of USB Drive mounted in windows explorer after plugging the device. Changed product string to \u0026ldquo;Nikhil Test Flash drive\u0026rdquo;. OTi2272 datasheet List of Other Flash Memory Controller used for Testing CBM1180 USB2.0 Controller SM3257 USB2.0 Controller IS917 USB3.0 Controller Test PCBA The test PCB is divided into two parts\nOne PCB has OTi Controller, USB connector and other components. Another PCB has NAND flash memory. This PCB plugs into the controller PCB. It it designed to test different NAND flash memories. PCB for testing other controller Mass Storage circuit used in product PCB Demo Video ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/mass-storage/","summary":"Description Many products requires a onboard storage memory to store different data like drivers, manuals so that user can access it over the USB connection. The system should be access over a USB host computer. It should mount as a mass storage device (like drive).\nTo system main parts\nNAND Flash Memory: NAND flash memory is a type of non-volatile storage technology that does not require power in order to retain data.","title":"USB Mass Storage Controller (Flash Drive)"},{"content":"\rProject Description The device is a USB to Dual HDMI converter. The device is used to connect two HDMI monitor to a PC over the USB. This system can be used in the USB Docking station.\nWorking The device uses a Silicon Motion SM768 USB Display Docking Station SoC. SM768 USB display docking station graphic SoC enables multiple displays up to 4K ultra high definition through USB interface. Device can instantly mirror their computer screen to any external displays without the need to install a display driver. SM768 combine high graphics performance with low CPU loading and low power consumption. The CAT compression technology offers a high compression ratio while maintaining great image/video quality.\nSM768 has a one direct HDMI Output Port. For the another HDMI port output, SM768 is interfaced with a HDMI transmitter IC (ITI66121FN). The transmitter IC converts the SM768 TTL data into HDMI data. A Nand Flash memory is used to store the SM768 firmware.\nThe device is connected to a host PC overt the USB3.0 interface.\nBlock Diagram PCB Designing The SM768 is a 425 pin BGA SoC. A 4 layer impedance controlled PCB is used for the device.\n100 Ohm Impedance controlled differential pair tracks used for HDMI signals and 90 Ohm Impedance controlled differential pair tracks used for USB3.0 signals.\nSpecifications Two HDMI 1.4 Output Display Resolution 4K (3840x2160)@30Hz HD(1920x1080) \u0026amp; 2K @60Hz USB3.0 Micro B Connector Software Support Windows 7/8/10/11 \u0026amp; Servers, 32-bit, and 64-bit macOS X Linux OS(Ubuntu, RedHat, SUSE, etc) Android No Driver Required Custom Startup Logo Photo 4 Layer PCB Top Side Bottom Side Video ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/1k_usb_dock/","summary":"Project Description The device is a USB to Dual HDMI converter. The device is used to connect two HDMI monitor to a PC over the USB. This system can be used in the USB Docking station.\nWorking The device uses a Silicon Motion SM768 USB Display Docking Station SoC. SM768 USB display docking station graphic SoC enables multiple displays up to 4K ultra high definition through USB interface. Device can instantly mirror their computer screen to any external displays without the need to install a display driver.","title":"USB to Dual HDMI Converter (HDMI Dock)"},{"content":" Description The USB 2.0 is a 4 core cable.(VCC,GND,D+,D-)\nThe USB Hub system consist of below blocks\nUpstream Port (connection with the host PC) Downstream port (connection with the peripherals) Main USB HUB controller (manages the communication between peripheral devices and the computer system) Overcurrent protection IC (Protect the downstream port from short circuit and overcurrnet). ESD protection for downstream and upstream port (To protect the circuit from ESD) The USB HUB controller IC has different variants with 2,3,4 and 8 downstream ports. Below are the list of USB2.0 Hub ICs that we used\nUSB2512B 2 Downstream Port 36 pin QFN Package USB2514B 4 Downstream Port 36 pin QFN Package USB2640 2 Downstream Port QFN Package Flash memory card support FE1.1 4 Downstream port PCB Layout A PCB layout design guideline has to be followed while designing the high speed pcb to maintain signal integrity and avoid EMC issues.\nPCB Details\n4 Layer Impedance controlled PCB. USB2.0 D+/D- differntial pair routing D+/D- trace impedance : 90 Ohm USB Hub used on PCB ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/usb-hubs/usb2.0-hub/","summary":"Description The USB 2.0 is a 4 core cable.(VCC,GND,D+,D-)\nThe USB Hub system consist of below blocks\nUpstream Port (connection with the host PC) Downstream port (connection with the peripherals) Main USB HUB controller (manages the communication between peripheral devices and the computer system) Overcurrent protection IC (Protect the downstream port from short circuit and overcurrnet). ESD protection for downstream and upstream port (To protect the circuit from ESD) The USB HUB controller IC has different variants with 2,3,4 and 8 downstream ports.","title":"USB2.0 HUB"},{"content":" Description A USB hub is a device that expands a single Universal Serial Bus (USB) port into several so that there are more ports available to connect devices to a host system. All devices connected through a USB hub share the bandwidth available to that hub. The HUB block is used in the many products for connecting multiple USB peripherals.\nBelow is the list of Hubs\n\u0026ndash;\u0026gt; USB HUB List \u0026lt;\u0026ndash; ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/usb-hub/","summary":" Description A USB hub is a device that expands a single Universal Serial Bus (USB) port into several so that there are more ports available to connect devices to a host system. All devices connected through a USB hub share the bandwidth available to that hub. The HUB block is used in the many products for connecting multiple USB peripherals.\nBelow is the list of Hubs\n\u0026ndash;\u0026gt; USB HUB List \u0026lt;\u0026ndash; ","title":"USB3.2 and USB2.0 HUB"},{"content":" Project Description Wheelchair is used to provide a mode of transportation for such disabled people with impairments in hands and legs. People with such issues like paralytic people find it difficult to operate the wheelchair manually or using a remote assembly. For such people the project is designed to work on voice based commands so that the paralytic or disabled person can give direction commands by just speaking into the microphone given. The system also controlled by the head movement and joystick. It also has obstacle detection sensor which stops the wheelchair if any person/object comes in front of wheelchair.\nWorking The system consist of an Atmega32 based circuit interfaced with an voice recognition module that takes speech commands from the user converts this speech into digital data which is then debugged by the micro-controller to get directional commands. A BLDC motors are uses to drive the wheelchair. An ultrasonic sensor are used for obstacle detection and accelerometer is used for gesture detection. A touchscreen is provided for setting the wheelchair parameters like wheelchair speed, changing the operating mode and to indicate the status like which side obstacle sensor is triggered. The system is powered using lead acid battery.\nThe user can program the wheelchair voice command in any language. Currently the voice commands are programmed in Marathi language.\nThe Project is applied for the Patent in India. Patent Application No: 3209/MUM/2013\nSpecifications Controlling: Voice Command, Gesture Control, Joystick Control, Touch screen control. Obstacle detection. Voice module (HM2007) Atemga32 / TI Controller 48V BLDC Motor 48V Lead Acid Battery Gesture Sensor: ADXL335 Accelerometer Obstacle Sensor: HC-SR04 Ultrasonic Sensor. Display: 128x 64 Graphical display. Touch: 4 wire Resistive touch Video ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/voice-controller-wheelchair/","summary":"Project Description Wheelchair is used to provide a mode of transportation for such disabled people with impairments in hands and legs. People with such issues like paralytic people find it difficult to operate the wheelchair manually or using a remote assembly. For such people the project is designed to work on voice based commands so that the paralytic or disabled person can give direction commands by just speaking into the microphone given.","title":"Voice \u0026 Gesture Controlled Wheelchair"},{"content":" Designed By CEMTREX Product Description cover the USB to I2S DAC part as well\nWorking Techincal Specifications single channel daddw\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/wireless-module/","summary":"Designed By CEMTREX Product Description cover the USB to I2S DAC part as well\nWorking Techincal Specifications single channel daddw","title":"Wireless Modules"},{"content":" Designed By CEMTREX Product Description cover the USB to I2S DAC part as well\nWorking Techincal Specifications single channel daddw\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/projects/single-board-computer/","summary":"Designed By CEMTREX Product Description cover the USB to I2S DAC part as well\nWorking Techincal Specifications single channel daddw","title":"Worked with SBCs"},{"content":"Credentials 🔗 Certificate 🔗 Score Report 🔗 Credly Badge Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with a score of 962/1000 with just 2 months of preparation while working full-time as a software engineer. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.\nWhat is AWS Solutions Architect Associate Certification With the rise of Cloud Computing, companies have constantly been shifting from running their infrastructure on-premise, to running them on cloud, which offers far more elasticity in terms of scaling and resiliency in case a disaster strikes.\n{{/\u0026lt; gallery directory=\u0026ldquo;images/*\u0026rdquo; \u0026gt;}}\nThe AWS Solutions Architect - Associate certification validates your ability to design and deploy well-architected solutions on AWS, which is the leading cloud provider today. In simple terms, this exam tests your ability to propose an architecture given a specific scenario. For example: a company wants their application to continue running even if an entire AWS region, where that application was hosted, is down due to a disaster. So, how would you design their infrastructure around this use case?\nExam Format In this exam, you will get 65 questions and 130 minutes to answer them which means you get 2 mins per question. You will be graded on a percentile basis on a scale of 100 to 1000 where you will nead to score more than 720 to pass the exam. Based on this information, you can approximate that you will have to answer about 72% of the questions correctly in order to pass. You can refer this link for more details on how scoring works.\nThis exam has a pass / fail criteria. If you pass the exam, your score doesn\u0026rsquo;t really matter. It will only be written on your score report for your reference. It will not be mentioned anywhere on the certificate.\nThe cost of taking this exam is 150 USD and with taxes it comes up to about 177 USD. Since I took the exam in India, the amount for me was around 13,500 INR.\nMy Preparation Strategy I prepared for about 2 months while having a full-time job and simultaneously running my 🎬 \u0026lt;strong\u0026gt;YouTube channel\u0026lt;/strong\u0026gt; where I post resourceful videos every week. During these 2 months of preparatory period, I studied for about 2 hours on weekdays and 4-6 hours on weekends.\nFor preparation, I took the AWS Solutions Architect Associate Course by \u0026lt;strong\u0026gt;Stephane Maarek\u0026lt;/strong\u0026gt; which is available on Udemy. While taking this course, I dumped all of the information available in the course into a \u0026lt;strong\u0026gt;Notion\u0026lt;/strong\u0026gt; page.\nOnce I was done with the course, I bought three practice test packages for the AWS SAA exam on Udemy that are provided by:\nStephane Maarek Jon Bonso Neal Davis Each of these practice test packages contain 6 practice tests. Additionally, a free practice test is provided with the course.\nWhile taking the practice tests, I dumped the questions along with their explanations, into a Notion page, for the questions that I got wrong and the questions that I found difficult to answer. This would come in handy later when I revise everything before the exam.\nUntil now, everything was entangled in my head as I had not consumed information in an organized manner. So, I consolidated all of the information from the course and the practice tests into dense concise notes that, instead of Notion, I took on another note-taking app called Obsidian. I’ll explain why in another video. For the sake of the AWS exam, you can take your notes anywhere.\nIf you want my notes, you will have to wait for some time until I figure out a way to share my Obsidian notes in a presentable format. They cannot be directly shared like Notion pages.\nConsolidating my notes took about a week and while doing so I went through all of the information again but this time with a much more idea of the concepts. Everything started making sense and I felt confident to take the AWS SAA exam. So, I revised my consolidated notes once and took the exam the next day.\nTaking the Test You can either take the test offline at a testing center or online at the comfort of your home. I would suggest you take the test offline if you have testing centers in your area. If not, then you can take the online route. I had to take this test online as there are no testing centers nearby.\nDo keep in mind that the proctoring in the online test is extremely strict and if the proctor cancels your exam, you won\u0026rsquo;t get a refund. In such a scenario, you will have to rebook and retake the test at a later date.\nTips for taking the AWS SAA exam Take as many practice tests as you can. They will give you an idea of the kind of topics that come up in the exam most often. Also, the questions in these practice tests match very well with the ones appearing on the actual exam.\nThe amount of information that you will have to go through to prepare for this exam is enormous. You not only need a good understanding of the various AWS resources and architectures, but you will also have to remember a lot of information. So, filtering out the irrelevant details from the dumped information and making concise notes, that you can easily revise within 1 or 2 days is crucial for this exam.\nThat\u0026rsquo;s all folks That was all about the AWS Solutions Architect - Associate exam. Up next, I have plans to take the AWS Developer Associate exam which focuses on the development aroud AWS services. As a personal milestone, I want to clear the AWS Developer Associate exam before I move to Canada 🇨🇦 for my MS.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/blog/ac-dimmer/","summary":"Credentials 🔗 Certificate 🔗 Score Report 🔗 Credly Badge Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with a score of 962/1000 with just 2 months of preparation while working full-time as a software engineer. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.","title":"ac dimmer "},{"content":"Credentials 🔗 Certificate 🔗 Score Report 🔗 Credly Badge Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with a score of 962/1000 with just 2 months of preparation while working full-time as a software engineer. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.\nWhat is AWS Solutions Architect Associate Certification With the rise of Cloud Computing, companies have constantly been shifting from running their infrastructure on-premise, to running them on cloud, which offers far more elasticity in terms of scaling and resiliency in case a disaster strikes.\n{{/\u0026lt; gallery directory=\u0026ldquo;images/*\u0026rdquo; \u0026gt;}}\nThe AWS Solutions Architect - Associate certification validates your ability to design and deploy well-architected solutions on AWS, which is the leading cloud provider today. In simple terms, this exam tests your ability to propose an architecture given a specific scenario. For example: a company wants their application to continue running even if an entire AWS region, where that application was hosted, is down due to a disaster. So, how would you design their infrastructure around this use case?\nExam Format In this exam, you will get 65 questions and 130 minutes to answer them which means you get 2 mins per question. You will be graded on a percentile basis on a scale of 100 to 1000 where you will nead to score more than 720 to pass the exam. Based on this information, you can approximate that you will have to answer about 72% of the questions correctly in order to pass. You can refer this link for more details on how scoring works.\nThis exam has a pass / fail criteria. If you pass the exam, your score doesn\u0026rsquo;t really matter. It will only be written on your score report for your reference. It will not be mentioned anywhere on the certificate.\nThe cost of taking this exam is 150 USD and with taxes it comes up to about 177 USD. Since I took the exam in India, the amount for me was around 13,500 INR.\nMy Preparation Strategy I prepared for about 2 months while having a full-time job and simultaneously running my 🎬 \u0026lt;strong\u0026gt;YouTube channel\u0026lt;/strong\u0026gt; where I post resourceful videos every week. During these 2 months of preparatory period, I studied for about 2 hours on weekdays and 4-6 hours on weekends.\nFor preparation, I took the AWS Solutions Architect Associate Course by \u0026lt;strong\u0026gt;Stephane Maarek\u0026lt;/strong\u0026gt; which is available on Udemy. While taking this course, I dumped all of the information available in the course into a \u0026lt;strong\u0026gt;Notion\u0026lt;/strong\u0026gt; page.\nOnce I was done with the course, I bought three practice test packages for the AWS SAA exam on Udemy that are provided by:\nStephane Maarek Jon Bonso Neal Davis Each of these practice test packages contain 6 practice tests. Additionally, a free practice test is provided with the course.\nWhile taking the practice tests, I dumped the questions along with their explanations, into a Notion page, for the questions that I got wrong and the questions that I found difficult to answer. This would come in handy later when I revise everything before the exam.\nUntil now, everything was entangled in my head as I had not consumed information in an organized manner. So, I consolidated all of the information from the course and the practice tests into dense concise notes that, instead of Notion, I took on another note-taking app called Obsidian. I’ll explain why in another video. For the sake of the AWS exam, you can take your notes anywhere.\nIf you want my notes, you will have to wait for some time until I figure out a way to share my Obsidian notes in a presentable format. They cannot be directly shared like Notion pages.\nConsolidating my notes took about a week and while doing so I went through all of the information again but this time with a much more idea of the concepts. Everything started making sense and I felt confident to take the AWS SAA exam. So, I revised my consolidated notes once and took the exam the next day.\nTaking the Test You can either take the test offline at a testing center or online at the comfort of your home. I would suggest you take the test offline if you have testing centers in your area. If not, then you can take the online route. I had to take this test online as there are no testing centers nearby.\nDo keep in mind that the proctoring in the online test is extremely strict and if the proctor cancels your exam, you won\u0026rsquo;t get a refund. In such a scenario, you will have to rebook and retake the test at a later date.\nTips for taking the AWS SAA exam Take as many practice tests as you can. They will give you an idea of the kind of topics that come up in the exam most often. Also, the questions in these practice tests match very well with the ones appearing on the actual exam.\nThe amount of information that you will have to go through to prepare for this exam is enormous. You not only need a good understanding of the various AWS resources and architectures, but you will also have to remember a lot of information. So, filtering out the irrelevant details from the dumped information and making concise notes, that you can easily revise within 1 or 2 days is crucial for this exam.\nThat\u0026rsquo;s all folks That was all about the AWS Solutions Architect - Associate exam. Up next, I have plans to take the AWS Developer Associate exam which focuses on the development aroud AWS services. As a personal milestone, I want to clear the AWS Developer Associate exam before I move to Canada 🇨🇦 for my MS.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/blog/cp2102-custom-driver/","summary":"Credentials 🔗 Certificate 🔗 Score Report 🔗 Credly Badge Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with a score of 962/1000 with just 2 months of preparation while working full-time as a software engineer. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.","title":"CP2102 custom driver name"},{"content":"Credentials 🔗 Certificate 🔗 Score Report 🔗 Credly Badge Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with a score of 962/1000 with just 2 months of preparation while working full-time as a software engineer. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.\nWhat is AWS Solutions Architect Associate Certification With the rise of Cloud Computing, companies have constantly been shifting from running their infrastructure on-premise, to running them on cloud, which offers far more elasticity in terms of scaling and resiliency in case a disaster strikes.\n{{/\u0026lt; gallery directory=\u0026ldquo;images/*\u0026rdquo; \u0026gt;}}\nThe AWS Solutions Architect - Associate certification validates your ability to design and deploy well-architected solutions on AWS, which is the leading cloud provider today. In simple terms, this exam tests your ability to propose an architecture given a specific scenario. For example: a company wants their application to continue running even if an entire AWS region, where that application was hosted, is down due to a disaster. So, how would you design their infrastructure around this use case?\nExam Format In this exam, you will get 65 questions and 130 minutes to answer them which means you get 2 mins per question. You will be graded on a percentile basis on a scale of 100 to 1000 where you will nead to score more than 720 to pass the exam. Based on this information, you can approximate that you will have to answer about 72% of the questions correctly in order to pass. You can refer this link for more details on how scoring works.\nThis exam has a pass / fail criteria. If you pass the exam, your score doesn\u0026rsquo;t really matter. It will only be written on your score report for your reference. It will not be mentioned anywhere on the certificate.\nThe cost of taking this exam is 150 USD and with taxes it comes up to about 177 USD. Since I took the exam in India, the amount for me was around 13,500 INR.\nMy Preparation Strategy I prepared for about 2 months while having a full-time job and simultaneously running my 🎬 \u0026lt;strong\u0026gt;YouTube channel\u0026lt;/strong\u0026gt; where I post resourceful videos every week. During these 2 months of preparatory period, I studied for about 2 hours on weekdays and 4-6 hours on weekends.\nFor preparation, I took the AWS Solutions Architect Associate Course by \u0026lt;strong\u0026gt;Stephane Maarek\u0026lt;/strong\u0026gt; which is available on Udemy. While taking this course, I dumped all of the information available in the course into a \u0026lt;strong\u0026gt;Notion\u0026lt;/strong\u0026gt; page.\nOnce I was done with the course, I bought three practice test packages for the AWS SAA exam on Udemy that are provided by:\nStephane Maarek Jon Bonso Neal Davis Each of these practice test packages contain 6 practice tests. Additionally, a free practice test is provided with the course.\nWhile taking the practice tests, I dumped the questions along with their explanations, into a Notion page, for the questions that I got wrong and the questions that I found difficult to answer. This would come in handy later when I revise everything before the exam.\nUntil now, everything was entangled in my head as I had not consumed information in an organized manner. So, I consolidated all of the information from the course and the practice tests into dense concise notes that, instead of Notion, I took on another note-taking app called Obsidian. I’ll explain why in another video. For the sake of the AWS exam, you can take your notes anywhere.\nIf you want my notes, you will have to wait for some time until I figure out a way to share my Obsidian notes in a presentable format. They cannot be directly shared like Notion pages.\nConsolidating my notes took about a week and while doing so I went through all of the information again but this time with a much more idea of the concepts. Everything started making sense and I felt confident to take the AWS SAA exam. So, I revised my consolidated notes once and took the exam the next day.\nTaking the Test You can either take the test offline at a testing center or online at the comfort of your home. I would suggest you take the test offline if you have testing centers in your area. If not, then you can take the online route. I had to take this test online as there are no testing centers nearby.\nDo keep in mind that the proctoring in the online test is extremely strict and if the proctor cancels your exam, you won\u0026rsquo;t get a refund. In such a scenario, you will have to rebook and retake the test at a later date.\nTips for taking the AWS SAA exam Take as many practice tests as you can. They will give you an idea of the kind of topics that come up in the exam most often. Also, the questions in these practice tests match very well with the ones appearing on the actual exam.\nThe amount of information that you will have to go through to prepare for this exam is enormous. You not only need a good understanding of the various AWS resources and architectures, but you will also have to remember a lot of information. So, filtering out the irrelevant details from the dumped information and making concise notes, that you can easily revise within 1 or 2 days is crucial for this exam.\nThat\u0026rsquo;s all folks That was all about the AWS Solutions Architect - Associate exam. Up next, I have plans to take the AWS Developer Associate exam which focuses on the development aroud AWS services. As a personal milestone, I want to clear the AWS Developer Associate exam before I move to Canada 🇨🇦 for my MS.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/blog/usb-flash-drive/","summary":"Credentials 🔗 Certificate 🔗 Score Report 🔗 Credly Badge Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with a score of 962/1000 with just 2 months of preparation while working full-time as a software engineer. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.","title":"Design a USB2.0 Flash Drive"},{"content":"Credentials 🔗 Certificate 🔗 Score Report 🔗 Credly Badge Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with a score of 962/1000 with just 2 months of preparation while working full-time as a software engineer. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.\nWhat is AWS Solutions Architect Associate Certification With the rise of Cloud Computing, companies have constantly been shifting from running their infrastructure on-premise, to running them on cloud, which offers far more elasticity in terms of scaling and resiliency in case a disaster strikes.\n{{/\u0026lt; gallery directory=\u0026ldquo;images/*\u0026rdquo; \u0026gt;}}\nThe AWS Solutions Architect - Associate certification validates your ability to design and deploy well-architected solutions on AWS, which is the leading cloud provider today. In simple terms, this exam tests your ability to propose an architecture given a specific scenario. For example: a company wants their application to continue running even if an entire AWS region, where that application was hosted, is down due to a disaster. So, how would you design their infrastructure around this use case?\nExam Format In this exam, you will get 65 questions and 130 minutes to answer them which means you get 2 mins per question. You will be graded on a percentile basis on a scale of 100 to 1000 where you will nead to score more than 720 to pass the exam. Based on this information, you can approximate that you will have to answer about 72% of the questions correctly in order to pass. You can refer this link for more details on how scoring works.\nThis exam has a pass / fail criteria. If you pass the exam, your score doesn\u0026rsquo;t really matter. It will only be written on your score report for your reference. It will not be mentioned anywhere on the certificate.\nThe cost of taking this exam is 150 USD and with taxes it comes up to about 177 USD. Since I took the exam in India, the amount for me was around 13,500 INR.\nMy Preparation Strategy I prepared for about 2 months while having a full-time job and simultaneously running my 🎬 \u0026lt;strong\u0026gt;YouTube channel\u0026lt;/strong\u0026gt; where I post resourceful videos every week. During these 2 months of preparatory period, I studied for about 2 hours on weekdays and 4-6 hours on weekends.\nFor preparation, I took the AWS Solutions Architect Associate Course by \u0026lt;strong\u0026gt;Stephane Maarek\u0026lt;/strong\u0026gt; which is available on Udemy. While taking this course, I dumped all of the information available in the course into a \u0026lt;strong\u0026gt;Notion\u0026lt;/strong\u0026gt; page.\nOnce I was done with the course, I bought three practice test packages for the AWS SAA exam on Udemy that are provided by:\nStephane Maarek Jon Bonso Neal Davis Each of these practice test packages contain 6 practice tests. Additionally, a free practice test is provided with the course.\nWhile taking the practice tests, I dumped the questions along with their explanations, into a Notion page, for the questions that I got wrong and the questions that I found difficult to answer. This would come in handy later when I revise everything before the exam.\nUntil now, everything was entangled in my head as I had not consumed information in an organized manner. So, I consolidated all of the information from the course and the practice tests into dense concise notes that, instead of Notion, I took on another note-taking app called Obsidian. I’ll explain why in another video. For the sake of the AWS exam, you can take your notes anywhere.\nIf you want my notes, you will have to wait for some time until I figure out a way to share my Obsidian notes in a presentable format. They cannot be directly shared like Notion pages.\nConsolidating my notes took about a week and while doing so I went through all of the information again but this time with a much more idea of the concepts. Everything started making sense and I felt confident to take the AWS SAA exam. So, I revised my consolidated notes once and took the exam the next day.\nTaking the Test You can either take the test offline at a testing center or online at the comfort of your home. I would suggest you take the test offline if you have testing centers in your area. If not, then you can take the online route. I had to take this test online as there are no testing centers nearby.\nDo keep in mind that the proctoring in the online test is extremely strict and if the proctor cancels your exam, you won\u0026rsquo;t get a refund. In such a scenario, you will have to rebook and retake the test at a later date.\nTips for taking the AWS SAA exam Take as many practice tests as you can. They will give you an idea of the kind of topics that come up in the exam most often. Also, the questions in these practice tests match very well with the ones appearing on the actual exam.\nThe amount of information that you will have to go through to prepare for this exam is enormous. You not only need a good understanding of the various AWS resources and architectures, but you will also have to remember a lot of information. So, filtering out the irrelevant details from the dumped information and making concise notes, that you can easily revise within 1 or 2 days is crucial for this exam.\nThat\u0026rsquo;s all folks That was all about the AWS Solutions Architect - Associate exam. Up next, I have plans to take the AWS Developer Associate exam which focuses on the development aroud AWS services. As a personal milestone, I want to clear the AWS Developer Associate exam before I move to Canada 🇨🇦 for my MS.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/blog/esp32-programmer/","summary":"Credentials 🔗 Certificate 🔗 Score Report 🔗 Credly Badge Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with a score of 962/1000 with just 2 months of preparation while working full-time as a software engineer. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.","title":"ESP32 Windows USB programmer"},{"content":"Credentials 🔗 Certificate 🔗 Score Report 🔗 Credly Badge Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with a score of 962/1000 with just 2 months of preparation while working full-time as a software engineer. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.\nWhat is AWS Solutions Architect Associate Certification With the rise of Cloud Computing, companies have constantly been shifting from running their infrastructure on-premise, to running them on cloud, which offers far more elasticity in terms of scaling and resiliency in case a disaster strikes.\n{{/\u0026lt; gallery directory=\u0026ldquo;images/*\u0026rdquo; \u0026gt;}}\nThe AWS Solutions Architect - Associate certification validates your ability to design and deploy well-architected solutions on AWS, which is the leading cloud provider today. In simple terms, this exam tests your ability to propose an architecture given a specific scenario. For example: a company wants their application to continue running even if an entire AWS region, where that application was hosted, is down due to a disaster. So, how would you design their infrastructure around this use case?\nExam Format In this exam, you will get 65 questions and 130 minutes to answer them which means you get 2 mins per question. You will be graded on a percentile basis on a scale of 100 to 1000 where you will nead to score more than 720 to pass the exam. Based on this information, you can approximate that you will have to answer about 72% of the questions correctly in order to pass. You can refer this link for more details on how scoring works.\nThis exam has a pass / fail criteria. If you pass the exam, your score doesn\u0026rsquo;t really matter. It will only be written on your score report for your reference. It will not be mentioned anywhere on the certificate.\nThe cost of taking this exam is 150 USD and with taxes it comes up to about 177 USD. Since I took the exam in India, the amount for me was around 13,500 INR.\nMy Preparation Strategy I prepared for about 2 months while having a full-time job and simultaneously running my 🎬 \u0026lt;strong\u0026gt;YouTube channel\u0026lt;/strong\u0026gt; where I post resourceful videos every week. During these 2 months of preparatory period, I studied for about 2 hours on weekdays and 4-6 hours on weekends.\nFor preparation, I took the AWS Solutions Architect Associate Course by \u0026lt;strong\u0026gt;Stephane Maarek\u0026lt;/strong\u0026gt; which is available on Udemy. While taking this course, I dumped all of the information available in the course into a \u0026lt;strong\u0026gt;Notion\u0026lt;/strong\u0026gt; page.\nOnce I was done with the course, I bought three practice test packages for the AWS SAA exam on Udemy that are provided by:\nStephane Maarek Jon Bonso Neal Davis Each of these practice test packages contain 6 practice tests. Additionally, a free practice test is provided with the course.\nWhile taking the practice tests, I dumped the questions along with their explanations, into a Notion page, for the questions that I got wrong and the questions that I found difficult to answer. This would come in handy later when I revise everything before the exam.\nUntil now, everything was entangled in my head as I had not consumed information in an organized manner. So, I consolidated all of the information from the course and the practice tests into dense concise notes that, instead of Notion, I took on another note-taking app called Obsidian. I’ll explain why in another video. For the sake of the AWS exam, you can take your notes anywhere.\nIf you want my notes, you will have to wait for some time until I figure out a way to share my Obsidian notes in a presentable format. They cannot be directly shared like Notion pages.\nConsolidating my notes took about a week and while doing so I went through all of the information again but this time with a much more idea of the concepts. Everything started making sense and I felt confident to take the AWS SAA exam. So, I revised my consolidated notes once and took the exam the next day.\nTaking the Test You can either take the test offline at a testing center or online at the comfort of your home. I would suggest you take the test offline if you have testing centers in your area. If not, then you can take the online route. I had to take this test online as there are no testing centers nearby.\nDo keep in mind that the proctoring in the online test is extremely strict and if the proctor cancels your exam, you won\u0026rsquo;t get a refund. In such a scenario, you will have to rebook and retake the test at a later date.\nTips for taking the AWS SAA exam Take as many practice tests as you can. They will give you an idea of the kind of topics that come up in the exam most often. Also, the questions in these practice tests match very well with the ones appearing on the actual exam.\nThe amount of information that you will have to go through to prepare for this exam is enormous. You not only need a good understanding of the various AWS resources and architectures, but you will also have to remember a lot of information. So, filtering out the irrelevant details from the dumped information and making concise notes, that you can easily revise within 1 or 2 days is crucial for this exam.\nThat\u0026rsquo;s all folks That was all about the AWS Solutions Architect - Associate exam. Up next, I have plans to take the AWS Developer Associate exam which focuses on the development aroud AWS services. As a personal milestone, I want to clear the AWS Developer Associate exam before I move to Canada 🇨🇦 for my MS.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/blog/leg-remote-controller/","summary":"Credentials 🔗 Certificate 🔗 Score Report 🔗 Credly Badge Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with a score of 962/1000 with just 2 months of preparation while working full-time as a software engineer. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.","title":"Hacking Leg remote controller"},{"content":"About DDC The Display Data Channel, or DDC, is a collection of protocols for digital communication between a display and a graphics adapter that enable the display to communicate its supported display modes to the adapter and that enable the computer host to adjust monitor parameters, such as brightness and contrast.\nObjective The DDC commands uses the I2C protocol for the data communication. We have designed a USB to HDMI Dock and the dock SoC used on the doesn\u0026rsquo;t support the DDC commands for brightness and contrast adjustment . To provide the display brightness and contrast adjustment functionality in dock, we wanted to design a application that will send the I2C commands directly to the display. For the interfacing, we have USB to I2C adaptor to directly send the I2C commands from the computer\u0026rsquo;s USB port to the display. There is no need of intermediate microcontroller for the communication between the display and computer. Hardware Setup A CP2112 USB to I2C bridge module is used for the sending the I2C commands from Windows application to HDMI display. Bridge module is connected to a Logic level shifter module because the bridge modules works on 3.3V logic and HDMI I2C channels works on 5V logic. The Bridge module SDA and SCL pins are connected to the HDMI port SDA and SCL lines. A DDC supported HDMI display is used for testing. (Not all display supports the DDc commands). I2C commands VCP commands are used to set the parameters. There are two type of VCP function.\nGetVCP (To get the information from display like name, input mode, current brightness/contrast values) SetVCP (To set the display parameter like brightness/contrast, input type, volume). SetVCP data parameters 6E Destination address\r51 Source address\r84 Length\r03 Set VCP Feature COMMAND\rCP VCP opcode\rSH High byte\rSL Low byte\rCS CheckSum Brightness Control To send the I2C command from the computer, we have used the CP2112 HidSmbus Utiltiy provided by SilconLabs. Silsiocn Labs also provided the SDK that contains all the information for the CP2112 APIs to deveiop a Windows/Mac/Linux application. For the testing we have used the demo \u0026lsquo;HidSmbus example\u0026rsquo; application prvided bt Silicon Labs in the SDK.\nDownload the CP2112 SDK from here. Extract and open the HidSmbus example.exe from SiliconLabs\\USBXpressHostSDK\\CP2112\\Release\\x86\nAfter oprmimg the appication, select the connection (i.e. CP2112 module) and click on connect. Now click on the \u0026lsquo;Data Transfer\u0026rsquo; tab. The I2C address of the display is 0x37 (Slave address). But in the application we have to set it to 0x6E. (0x37 \u0026laquo; 1 = 6E) left shift 0x37 by one you will get 6E.\nNow to set the brightness we have to use the SetVCP commands. First we have to set the below data\ndata:6E 51 84 03 10\n6E Destination address\r51 Source address\r84 Length\r03 Set VCP Feature COMMAND\r10 VCP opcode for brightness The next two bytes are the brigntess value in hex. e.g for 100% brightness, Convert the 100 into hex which is 0x00 0x64. You can caluclute it using Windows programmer caluclator. 100(DEC) = 0x00 0x64 (Hex).\n00 High byte of brightness value\r64 Low byte of brightness value data: 6E 51 84 03 10 00 64\nand the last byte is checksum which can be calculate as below\n6E XOR 51 XOR 84 XOR 03 XOR 10 XOR 00 XOR 64 = CC\nNow the complete data is 51 84 03 10 00 64 CC\nClick on write request and send the data, The brightness of display should be change to 100%\nLike wise for 10% brightness the data is 51 84 03 10 00 0A A2\nWe have connected a logic analyzer to see what actaul data is wrtten on the I2C lines. Contrast Control To change the contrast set the VCP opcode to 0x12. So for 100% contrast the data is 51 84 03 12 00 64 CE\nIn this way by changing the opcode we can set/control other display settings like tuning ON/OFF display, selecting color preset, changing input mode.\nYou can find differnt opcodes and detailed infomation of DDC commands below. https://milek7.pl/ddcbacklight/ddcci.pdf https://www.ddcutil.com/getvcp_known_u3011_output/\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/blog/ddc_brightness_control/","summary":"About DDC The Display Data Channel, or DDC, is a collection of protocols for digital communication between a display and a graphics adapter that enable the display to communicate its supported display modes to the adapter and that enable the computer host to adjust monitor parameters, such as brightness and contrast.\nObjective The DDC commands uses the I2C protocol for the data communication. We have designed a USB to HDMI Dock and the dock SoC used on the doesn\u0026rsquo;t support the DDC commands for brightness and contrast adjustment .","title":"HDMI DDC Brightness \u0026 Contrast Control"},{"content":"Credentials 🔗 Certificate 🔗 Credly Badge 🎬 YouTube Video Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with just a month of preparation while pursuing MSc in Applied Computing at the University of Toronto and working as a DevOps Intern at 16Bit, a med-tech startup based in Toronto. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.\nWhat is AWS Developer - Associate Certification With the rise of Cloud Computing, companies have constantly been shifting from running their infrastructure on-premise, which is hard to scale and manage, to running them on cloud, which offers far more elasticity in terms of scaling and resiliency in case of disasters.\nThe AWS Developer - Associate certification validates your expertise in developing and maintaining applications on AWS, which is the leading cloud provider today. It focuses on AWS core services, development tools and deployment practices. It can help developers enhance their skills, increase their earning potential, and demonstrate their proficiency to potential employers.\nExam Format In this exam, you will get 65 questions and 130 minutes to answer them which means you get 2 mins per question. Out of the total questions, 15 will be experimental (ungraded). But you will not know which ones are experimental, otherwise you will skip them. So, you need to attempt every question on the test.\nYou will be graded on a percentile basis on a scale of 100 to 1000 where you will nead to score more than 720 to pass the exam. Based on this information, you can approximate that you will have to answer about 72% of the questions correctly in order to pass. You can refer this link for more details on how scoring works.\nThis exam has a pass / fail criteria. If you pass the exam, your score doesn\u0026rsquo;t really matter. It will only be written on your score report for your reference. It will not be mentioned anywhere on the certificate.\nThe cost of taking this exam is 150 USD but I got 50% off, so I paid just 75 USD. The way it works is that if you passed your first AWS certification exam, then you can get 50% off on the second one if you take it within a year of taking the first certification. Since, I passed the AWS Solution’s Architect - Associate Certification exam in the month of May last year, I was eligible for the discount.\nMy Preparation Strategy I prepared for about 4-6 hours everyday for a months while being a grad student and working as a DevOps intern. I took the AWS Developer - Associate Course by \u0026lt;strong\u0026gt;Stephane Maarek\u0026lt;/strong\u0026gt; which is available on Udemy. Preparing for the DVA exam was a bit easy for me as I had already passed the SAA exam, because the concepts overlap to some degree between the two certification exams.\nOnce I was done with the course, I bought three sets of practice tests on Udemy that are provided by:\nStephane Maarek Jon Bonso Neal Davis Each of these practice test packages contain 6 practice tests. Additionally, a free practice test is provided with the course.\nWhile going through the course, I took easy to follow notes in Notion. Additionally, while taking the practice tests, I saved the questions along with their explanations, into a Notion page, for the questions that I got wrong and the questions that I found difficult to answer. This would come in handy later when I revise everything before the exam.\nI also created a notion page for quick notes to include information that’s hard to remember, things like message retention duration of a Kinesis Data Stream or the IOPS limits for the different EBS volume types. I just glanced through this stuff before taking the actual test to have all that information fresh in my memory.\n🔗 My notes\nTaking the Test You can take the test either at a testing center or online at the comfort of your home. I would suggest you go for a testing center if it is available nearby. The experience is so smooth and you don\u0026rsquo;t have to worry about internet connectivity or proctoring issues. I took the test at a testing center and my experience was good.\nLast year, I took the AWS SAA exam online from my home and my experience was pretty bad. The proctor asked me not to move my lips while reading the questions. So, the proctoring was way too strict. I was stressed and mindful about my body position, and was not able to focus well on the actual test. Another problem with taking the test online is that if the proctor cancels your exam, you won\u0026rsquo;t get a refund. You will have to rebook and retake the test at a later date.\nTips for taking the AWS SAA exam Take as many practice tests as you can. They will give you an idea of the kind of topics that come up in the exam most often. Also, the questions in these practice tests match very well with the ones appearing on the actual exam.\nThe amount of information that you will have to go through to prepare for this exam is enormous. You not only need a good understanding of the various AWS resources and architectures, but you will also have to remember a lot of information. So, keep at least 3 days to revise your notes before the exam.\nThat\u0026rsquo;s all folks That was all about the AWS Developer - Associate Certification exam. Thanks a lot for reading!\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/blog/aws-dva-certification/","summary":"Credentials 🔗 Certificate 🔗 Credly Badge 🎬 YouTube Video Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with just a month of preparation while pursuing MSc in Applied Computing at the University of Toronto and working as a DevOps Intern at 16Bit, a med-tech startup based in Toronto. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.","title":"I passed the AWS DVA Certification Exam"},{"content":"Credentials 🔗 Certificate 🔗 Credly Badge 🎬 YouTube Video Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with just 2 months of preparation while working full-time as a software engineer. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.\nWhat is AWS Solutions Architect Associate Certification With the rise of Cloud Computing, companies have constantly been shifting from running their infrastructure on-premise, to running them on cloud, which offers far more elasticity in terms of scaling and resiliency in case a disaster strikes.\n{{/\u0026lt; gallery directory=\u0026ldquo;images/*\u0026rdquo; \u0026gt;}}\nThe AWS Solutions Architect - Associate certification validates your ability to design and deploy well-architected solutions on AWS, which is the leading cloud provider today. In simple terms, this exam tests your ability to propose an architecture given a specific scenario. For example: a company wants their application to continue running even if an entire AWS region, where that application was hosted, is down due to a disaster. So, how would you design their infrastructure around this use case?\nExam Format In this exam, you will get 65 questions and 130 minutes to answer them which means you get 2 mins per question. You will be graded on a percentile basis on a scale of 100 to 1000 where you will nead to score more than 720 to pass the exam. Based on this information, you can approximate that you will have to answer about 72% of the questions correctly in order to pass. You can refer this link for more details on how scoring works.\nThis exam has a pass / fail criteria. If you pass the exam, your score doesn\u0026rsquo;t really matter. It will only be written on your score report for your reference. It will not be mentioned anywhere on the certificate.\nThe cost of taking this exam is 150 USD and with taxes it comes up to about 177 USD. Since I took the exam in India, the amount for me was around 13,500 INR.\nMy Preparation Strategy I prepared for about 2 months while having a full-time job and simultaneously running my 🎬 \u0026lt;strong\u0026gt;YouTube channel\u0026lt;/strong\u0026gt; where I post resourceful videos every week. During these 2 months of preparatory period, I studied for about 2 hours on weekdays and 4-6 hours on weekends.\nFor preparation, I took the AWS Solutions Architect Associate Course by \u0026lt;strong\u0026gt;Stephane Maarek\u0026lt;/strong\u0026gt; which is available on Udemy. While taking this course, I dumped all of the information available in the course into a \u0026lt;strong\u0026gt;Notion\u0026lt;/strong\u0026gt; page.\nOnce I was done with the course, I bought three practice test packages for the AWS SAA exam on Udemy that are provided by:\nStephane Maarek Jon Bonso Neal Davis Each of these practice test packages contain 6 practice tests. Additionally, a free practice test is provided with the course.\nWhile taking the practice tests, I dumped the questions along with their explanations, into a Notion page, for the questions that I got wrong and the questions that I found difficult to answer. This would come in handy later when I revise everything before the exam.\nUntil now, everything was entangled in my head as I had not consumed information in an organized manner. So, I consolidated all of the information from the course and the practice tests into dense concise notes that, instead of Notion, I took on another note-taking app called Obsidian. I’ll explain why in another video. For the sake of the AWS exam, you can take your notes anywhere.\nIf you want my notes, you will have to wait for some time until I figure out a way to share my Obsidian notes in a presentable format. They cannot be directly shared like Notion pages.\nConsolidating my notes took about a week and while doing so I went through all of the information again but this time with a much more idea of the concepts. Everything started making sense and I felt confident to take the AWS SAA exam. So, I revised my consolidated notes once and took the exam the next day.\nTaking the Test You can either take the test offline at a testing center or online at the comfort of your home. I would suggest you take the test offline if you have testing centers in your area. If not, then you can take the online route. I had to take this test online as there are no testing centers nearby.\nDo keep in mind that the proctoring in the online test is extremely strict and if the proctor cancels your exam, you won\u0026rsquo;t get a refund. In such a scenario, you will have to rebook and retake the test at a later date.\nTips for taking the AWS SAA exam Take as many practice tests as you can. They will give you an idea of the kind of topics that come up in the exam most often. Also, the questions in these practice tests match very well with the ones appearing on the actual exam.\nThe amount of information that you will have to go through to prepare for this exam is enormous. You not only need a good understanding of the various AWS resources and architectures, but you will also have to remember a lot of information. So, filtering out the irrelevant details from the dumped information and making concise notes, that you can easily revise within 1 or 2 days is crucial for this exam.\nThat\u0026rsquo;s all folks That was all about the AWS Solutions Architect - Associate exam. Up next, I have plans to take the AWS Developer Associate exam which focuses on the development aroud AWS services. As a personal milestone, I want to clear the AWS Developer Associate exam before I move to Canada 🇨🇦 for my MS.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/blog/aws-saa-certification/","summary":"Credentials 🔗 Certificate 🔗 Credly Badge 🎬 YouTube Video Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with just 2 months of preparation while working full-time as a software engineer. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.\nWhat is AWS Solutions Architect Associate Certification With the rise of Cloud Computing, companies have constantly been shifting from running their infrastructure on-premise, to running them on cloud, which offers far more elasticity in terms of scaling and resiliency in case a disaster strikes.","title":"I passed the AWS SAA Certification Exam"},{"content":"Credentials 🔗 Certificate 🔗 Score Report 🔗 Credly Badge Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with a score of 962/1000 with just 2 months of preparation while working full-time as a software engineer. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.\nWhat is AWS Solutions Architect Associate Certification With the rise of Cloud Computing, companies have constantly been shifting from running their infrastructure on-premise, to running them on cloud, which offers far more elasticity in terms of scaling and resiliency in case a disaster strikes.\n{{/\u0026lt; gallery directory=\u0026ldquo;images/*\u0026rdquo; \u0026gt;}}\nThe AWS Solutions Architect - Associate certification validates your ability to design and deploy well-architected solutions on AWS, which is the leading cloud provider today. In simple terms, this exam tests your ability to propose an architecture given a specific scenario. For example: a company wants their application to continue running even if an entire AWS region, where that application was hosted, is down due to a disaster. So, how would you design their infrastructure around this use case?\nExam Format In this exam, you will get 65 questions and 130 minutes to answer them which means you get 2 mins per question. You will be graded on a percentile basis on a scale of 100 to 1000 where you will nead to score more than 720 to pass the exam. Based on this information, you can approximate that you will have to answer about 72% of the questions correctly in order to pass. You can refer this link for more details on how scoring works.\nThis exam has a pass / fail criteria. If you pass the exam, your score doesn\u0026rsquo;t really matter. It will only be written on your score report for your reference. It will not be mentioned anywhere on the certificate.\nThe cost of taking this exam is 150 USD and with taxes it comes up to about 177 USD. Since I took the exam in India, the amount for me was around 13,500 INR.\nMy Preparation Strategy I prepared for about 2 months while having a full-time job and simultaneously running my 🎬 \u0026lt;strong\u0026gt;YouTube channel\u0026lt;/strong\u0026gt; where I post resourceful videos every week. During these 2 months of preparatory period, I studied for about 2 hours on weekdays and 4-6 hours on weekends.\nFor preparation, I took the AWS Solutions Architect Associate Course by \u0026lt;strong\u0026gt;Stephane Maarek\u0026lt;/strong\u0026gt; which is available on Udemy. While taking this course, I dumped all of the information available in the course into a \u0026lt;strong\u0026gt;Notion\u0026lt;/strong\u0026gt; page.\nOnce I was done with the course, I bought three practice test packages for the AWS SAA exam on Udemy that are provided by:\nStephane Maarek Jon Bonso Neal Davis Each of these practice test packages contain 6 practice tests. Additionally, a free practice test is provided with the course.\nWhile taking the practice tests, I dumped the questions along with their explanations, into a Notion page, for the questions that I got wrong and the questions that I found difficult to answer. This would come in handy later when I revise everything before the exam.\nUntil now, everything was entangled in my head as I had not consumed information in an organized manner. So, I consolidated all of the information from the course and the practice tests into dense concise notes that, instead of Notion, I took on another note-taking app called Obsidian. I’ll explain why in another video. For the sake of the AWS exam, you can take your notes anywhere.\nIf you want my notes, you will have to wait for some time until I figure out a way to share my Obsidian notes in a presentable format. They cannot be directly shared like Notion pages.\nConsolidating my notes took about a week and while doing so I went through all of the information again but this time with a much more idea of the concepts. Everything started making sense and I felt confident to take the AWS SAA exam. So, I revised my consolidated notes once and took the exam the next day.\nTaking the Test You can either take the test offline at a testing center or online at the comfort of your home. I would suggest you take the test offline if you have testing centers in your area. If not, then you can take the online route. I had to take this test online as there are no testing centers nearby.\nDo keep in mind that the proctoring in the online test is extremely strict and if the proctor cancels your exam, you won\u0026rsquo;t get a refund. In such a scenario, you will have to rebook and retake the test at a later date.\nTips for taking the AWS SAA exam Take as many practice tests as you can. They will give you an idea of the kind of topics that come up in the exam most often. Also, the questions in these practice tests match very well with the ones appearing on the actual exam.\nThe amount of information that you will have to go through to prepare for this exam is enormous. You not only need a good understanding of the various AWS resources and architectures, but you will also have to remember a lot of information. So, filtering out the irrelevant details from the dumped information and making concise notes, that you can easily revise within 1 or 2 days is crucial for this exam.\nThat\u0026rsquo;s all folks That was all about the AWS Solutions Architect - Associate exam. Up next, I have plans to take the AWS Developer Associate exam which focuses on the development aroud AWS services. As a personal milestone, I want to clear the AWS Developer Associate exam before I move to Canada 🇨🇦 for my MS.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/blog/usb_audio_dac_name/","summary":"Credentials 🔗 Certificate 🔗 Score Report 🔗 Credly Badge Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with a score of 962/1000 with just 2 months of preparation while working full-time as a software engineer. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.","title":"PCM2704 USB Audio DAC Device Name Change"},{"content":"Credentials 🔗 Certificate 🔗 Score Report 🔗 Credly Badge Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with a score of 962/1000 with just 2 months of preparation while working full-time as a software engineer. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.\nWhat is AWS Solutions Architect Associate Certification With the rise of Cloud Computing, companies have constantly been shifting from running their infrastructure on-premise, to running them on cloud, which offers far more elasticity in terms of scaling and resiliency in case a disaster strikes.\n{{/\u0026lt; gallery directory=\u0026ldquo;images/*\u0026rdquo; \u0026gt;}}\nThe AWS Solutions Architect - Associate certification validates your ability to design and deploy well-architected solutions on AWS, which is the leading cloud provider today. In simple terms, this exam tests your ability to propose an architecture given a specific scenario. For example: a company wants their application to continue running even if an entire AWS region, where that application was hosted, is down due to a disaster. So, how would you design their infrastructure around this use case?\nExam Format In this exam, you will get 65 questions and 130 minutes to answer them which means you get 2 mins per question. You will be graded on a percentile basis on a scale of 100 to 1000 where you will nead to score more than 720 to pass the exam. Based on this information, you can approximate that you will have to answer about 72% of the questions correctly in order to pass. You can refer this link for more details on how scoring works.\nThis exam has a pass / fail criteria. If you pass the exam, your score doesn\u0026rsquo;t really matter. It will only be written on your score report for your reference. It will not be mentioned anywhere on the certificate.\nThe cost of taking this exam is 150 USD and with taxes it comes up to about 177 USD. Since I took the exam in India, the amount for me was around 13,500 INR.\nMy Preparation Strategy I prepared for about 2 months while having a full-time job and simultaneously running my 🎬 \u0026lt;strong\u0026gt;YouTube channel\u0026lt;/strong\u0026gt; where I post resourceful videos every week. During these 2 months of preparatory period, I studied for about 2 hours on weekdays and 4-6 hours on weekends.\nFor preparation, I took the AWS Solutions Architect Associate Course by \u0026lt;strong\u0026gt;Stephane Maarek\u0026lt;/strong\u0026gt; which is available on Udemy. While taking this course, I dumped all of the information available in the course into a \u0026lt;strong\u0026gt;Notion\u0026lt;/strong\u0026gt; page.\nOnce I was done with the course, I bought three practice test packages for the AWS SAA exam on Udemy that are provided by:\nStephane Maarek Jon Bonso Neal Davis Each of these practice test packages contain 6 practice tests. Additionally, a free practice test is provided with the course.\nWhile taking the practice tests, I dumped the questions along with their explanations, into a Notion page, for the questions that I got wrong and the questions that I found difficult to answer. This would come in handy later when I revise everything before the exam.\nUntil now, everything was entangled in my head as I had not consumed information in an organized manner. So, I consolidated all of the information from the course and the practice tests into dense concise notes that, instead of Notion, I took on another note-taking app called Obsidian. I’ll explain why in another video. For the sake of the AWS exam, you can take your notes anywhere.\nIf you want my notes, you will have to wait for some time until I figure out a way to share my Obsidian notes in a presentable format. They cannot be directly shared like Notion pages.\nConsolidating my notes took about a week and while doing so I went through all of the information again but this time with a much more idea of the concepts. Everything started making sense and I felt confident to take the AWS SAA exam. So, I revised my consolidated notes once and took the exam the next day.\nTaking the Test You can either take the test offline at a testing center or online at the comfort of your home. I would suggest you take the test offline if you have testing centers in your area. If not, then you can take the online route. I had to take this test online as there are no testing centers nearby.\nDo keep in mind that the proctoring in the online test is extremely strict and if the proctor cancels your exam, you won\u0026rsquo;t get a refund. In such a scenario, you will have to rebook and retake the test at a later date.\nTips for taking the AWS SAA exam Take as many practice tests as you can. They will give you an idea of the kind of topics that come up in the exam most often. Also, the questions in these practice tests match very well with the ones appearing on the actual exam.\nThe amount of information that you will have to go through to prepare for this exam is enormous. You not only need a good understanding of the various AWS resources and architectures, but you will also have to remember a lot of information. So, filtering out the irrelevant details from the dumped information and making concise notes, that you can easily revise within 1 or 2 days is crucial for this exam.\nThat\u0026rsquo;s all folks That was all about the AWS Solutions Architect - Associate exam. Up next, I have plans to take the AWS Developer Associate exam which focuses on the development aroud AWS services. As a personal milestone, I want to clear the AWS Developer Associate exam before I move to Canada 🇨🇦 for my MS.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/blog/reverse-polarity-protection/","summary":"Credentials 🔗 Certificate 🔗 Score Report 🔗 Credly Badge Introduction I\u0026rsquo;ve passed the AWS Solutions Architect - Associate certification exam with a score of 962/1000 with just 2 months of preparation while working full-time as a software engineer. In this article, I’ll be sharing everything about this exam, my preparation strategies and tips. So, if you have plans on taking this exam anytime soon, read this article till the end.","title":"Reverse polarity protection "},{"content":"✏️ Intro If you’re like me who loves reading books on Kindle, you might have wondered how you could extract your highlights in an organized way and save them as notes. At least I did. You see, I use Notion as my primary note-taking/productivity management app and I already have a database of all the books that I have read so far and also the ones that I am planning to read next.\nAnd since each of these book entries in the Notion database is a page in itself, I thought why not populate them with the highlights that I made in Kindle while reading them. The only problem was Kindle stores all of the highlights in a text file (My Clippings.txt) which as you can see contains a tonne of useless information like the book location, where the highlight was made, and when it was made.\nI needed to find a way to filter out the highlights, group them by the book title and send them to my Notion book database. Not only that, all of this should happen automatically with minimal human effort. So, over the past two weekends, I spent the majority of my time coding and I’m finally ready with an app that would allow readers to seamlessly transfer all of their highlights to Notion. Let’s take a look\u0026hellip;\n🤖 Node Environment You need a stable version of Node JS, installed locally, to run this app. I have tested this on Node versions 16 and 14, and it has worked flawlessly on both of them. So, before proceeding to the next steps, make sure you have a stable version of Node installed. I’m not going to explain the environment setup in this article because the installation process might differ for different operating systems. You can easily learn that on Google.\n⚙️ Setup Follow the steps given below to set up the Kindle to Notion app on your local system.\nCopy my Books Database Template to your Notion dashboard. The app requires some fields (Title, Author, and Book Name) to be present in the database in order for the highlight sync to work properly. So, you can either create your own database having these fields or you can just copy mine using the template I provided above.\nClone the GitHub Repository to your local system and install the dependencies.\ngit clone https://github.com/arkalim/kindle-to-notion.git\rcd kindle-to-notion\rnpm install Rename these files or folders by removing .example extensions as shown below. The original files/folders in my local repo contained data that was either sensitive or specific to my highlights. So, I created empty aliases of them with .example extensions and committed them to GitHub.\n‣ cache.example ➡ cache\n‣ data.example ➡ data\n‣ .env.example ➡ .env\nGet your Notion API key at the Notion Integrations page and create a new internal integration. Integrations allow us to access a portion of our Notion workspace using a secret token called the Notion API key (Internal Integration Token).\nGo to your Notion dashboard. Navigate to the Books database. Click on Share in the top right-hand corner and invite the integration you just created. This will allow the integration to edit the Books database using the Notion API key that we got in the previous step. Copy the link to the Notion Books database and extract the Database Id as shown below. The database id is nothing but all of the gibberish between the last / and the ?. This is required by the app to perform CRUD operations on this database. Original Link: https://www.notion.so/arkalim/346be84507ff482b80fceb4024deadc2?v=e868075eaf5749bc941e617e651295fb\rDatabase Id: 346be84507ff482b80fceb4024deadc2 So, now you have the Notion API key as well as the Database Id. Now, populate these variables in the .env file. Storing this sensitive information in .env ensures that it won’t get exposed to the rest of the world if you commit your local repo to GitHub as .gitignore has been configured to ignore .env during commits. NOTION_API_KEY=your-notion-api-key\rBOOK_DB_ID=your-book-database-id Connect your Kindle to your computer. Navigate to Kindle ➡ documents and copy My Clippings.txt. Replace my My Clippings.txt in resources folder with yours. 🔁 Sync Highlights Finally, we are at the end of the setup section. You are now ready to sync your Kindle highlights to Notion. Open a terminal in your local repository and run the following command to watch your highlights teleport!\nnpm start ❗️For Nerds Every highlight made on Kindle is appended at the end of My Clippings.txt. RegEx has been used extensively throughout the application to parse and filter this text file. cache is a folder that contains the local cache to prevent the app from resyncing old highlights. data is a folder that contains the API response logs. .env is a file containing the environment variables like the Notion API key and the Database Id. Book Name is used as the primary key to facilitate upsert operation in the Notion database. Book Name corresponds to the title of the book in My Clippings.txt. So, this field should be left untouched. However, the other fields like Title, Author, Date Started, Date Finished, Status, and Genre could be modified as per your wish. The app maintains a local cache in the file sync.json present in the cache folder. This JSON file is updated at the end of each sync. This is done to prevent the app from resyncing the old highlights. If no new highlights have been made, no sync takes place. In case you wish to sync every book all over again, you need to empty the array present in sync.json and delete all the highlights present in your Notion database before running the sync. Responses from Notion API calls are exported to files with .json extensions in data folder. This was done to mitigate the problem of effectively logging JSON objects in the console (terminal). That’s all folks! If you made it till here, hats off to you! In this article, we learned how to set up Kindle to Notion app on our local system and use it to sync our Kindle highlights to the Notion Books database. If you want me to write more detailed articles explaining the inner workings of this app, drop a comment below. I write articles regularly so you should consider following me to get more such articles in your feed. Thanks a lot for reading!\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/blog/kindle-to-notion/","summary":"✏️ Intro If you’re like me who loves reading books on Kindle, you might have wondered how you could extract your highlights in an organized way and save them as notes. At least I did. You see, I use Notion as my primary note-taking/productivity management app and I already have a database of all the books that I have read so far and also the ones that I am planning to read next.","title":"Kindle to Notion"},{"content":"Introduction Ever wondered how Instagram applies stunning filters to your face? The software detects key points on your face and projects a mask on top. This tutorial will guide you on how to build one such software using Pytorch.\nDataset In this tutorial, we will use the official DLib Dataset which contains 6666 images of varying dimensions. Additionally, labels_ibug_300W_train.xml (comes with the dataset) contains the coordinates of 68 landmarks for each face. The script below will download the dataset and unzip it in Colab Notebook.\nif not os.path.exists(\u0026#39;/content/ibug_300W_large_face_landmark_dataset\u0026#39;): !wget http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz !tar -xvzf \u0026#39;ibug_300W_large_face_landmark_dataset.tar.gz\u0026#39; !rm -r \u0026#39;ibug_300W_large_face_landmark_dataset.tar.gz\u0026#39; Here is a sample image from the dataset. We can see that the face occupies a very small fraction of the entire image. If we feed the full image to the neural network, it will also process the background (irrelevant information), making it difficult for the model to learn. Therefore, we need to crop the image and feed only the face portion.\nData Preprocessing To prevent the neural network from overfitting the training dataset, we need to randomly transform the dataset. We will apply the following operations to the training and validation dataset:\nSince the face occupies a very small portion of the entire image, crop the image and use only the face for training. Resize the cropped face into a (224x224) image. Randomly change the brightness and saturation of the resized face. Randomly rotate the face after the above three transformations. Convert the image and landmarks into torch tensors and normalize them between [-1, 1]. class Transforms(): def __init__(self): pass def rotate(self, image, landmarks, angle): angle = random.uniform(-angle, +angle) transformation_matrix = torch.tensor([ [+cos(radians(angle)), -sin(radians(angle))], [+sin(radians(angle)), +cos(radians(angle))] ]) image = imutils.rotate(np.array(image), angle) landmarks = landmarks - 0.5 new_landmarks = np.matmul(landmarks, transformation_matrix) new_landmarks = new_landmarks + 0.5 return Image.fromarray(image), new_landmarks def resize(self, image, landmarks, img_size): image = TF.resize(image, img_size) return image, landmarks def color_jitter(self, image, landmarks): color_jitter = transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1) image = color_jitter(image) return image, landmarks def crop_face(self, image, landmarks, crops): left = int(crops[\u0026#39;left\u0026#39;]) top = int(crops[\u0026#39;top\u0026#39;]) width = int(crops[\u0026#39;width\u0026#39;]) height = int(crops[\u0026#39;height\u0026#39;]) image = TF.crop(image, top, left, height, width) img_shape = np.array(image).shape landmarks = torch.tensor(landmarks) - torch.tensor([[left, top]]) landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]]) return image, landmarks def __call__(self, image, landmarks, crops): image = Image.fromarray(image) image, landmarks = self.crop_face(image, landmarks, crops) image, landmarks = self.resize(image, landmarks, (224, 224)) image, landmarks = self.color_jitter(image, landmarks) image, landmarks = self.rotate(image, landmarks, angle=10) image = TF.to_tensor(image) image = TF.normalize(image, [0.5], [0.5]) return image, landmarks Dataset Class Now that we have our transformations ready, let’s write our dataset class. The labels_ibug_300W_train.xml contains the image path, landmarks and coordinates for the bounding box (for cropping the face). We will store these values in lists to access them easily during training. In this tutorial, the neural network will be trained on grayscale images.\nclass FaceLandmarksDataset(Dataset): def __init__(self, transform=None): tree = ET.parse(\u0026#39;ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml\u0026#39;) root = tree.getroot() self.image_filenames = [] self.landmarks = [] self.crops = [] self.transform = transform self.root_dir = \u0026#39;ibug_300W_large_face_landmark_dataset\u0026#39; for filename in root[2]: self.image_filenames.append(os.path.join(self.root_dir, filename.attrib[\u0026#39;file\u0026#39;])) self.crops.append(filename[0].attrib) landmark = [] for num in range(68): x_coordinate = int(filename[0][num].attrib[\u0026#39;x\u0026#39;]) y_coordinate = int(filename[0][num].attrib[\u0026#39;y\u0026#39;]) landmark.append([x_coordinate, y_coordinate]) self.landmarks.append(landmark) self.landmarks = np.array(self.landmarks).astype(\u0026#39;float32\u0026#39;) assert len(self.image_filenames) == len(self.landmarks) def __len__(self): return len(self.image_filenames) def __getitem__(self, index): image = cv2.imread(self.image_filenames[index], 0) landmarks = self.landmarks[index] if self.transform: image, landmarks = self.transform(image, landmarks, self.crops[index]) landmarks = landmarks - 0.5 return image, landmarks dataset = FaceLandmarksDataset(Transforms()) Note: landmarks = landmarks - 0.5 is done to zero-centre the landmarks as zero-centred outputs are easier for the neural network to learn.\nThe output of the dataset after preprocessing will look something like this (landmarks have been plotted on the image).\nNeural Network We will use the ResNet18 as the basic framework. We need to modify the first and last layers to suit our purpose. In the first layer, we will make the input channel count as 1 for the neural network to accept grayscale images. Similarly, in the final layer, the output channel count should equal 68 * 2 = 136 for the model to predict the (x, y) coordinates of the 68 landmarks for each face.\nclass Network(nn.Module): def __init__(self,num_classes=136): super().__init__() self.model_name=\u0026#39;resnet18\u0026#39; self.model=models.resnet18() self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) self.model.fc=nn.Linear(self.model.fc.in_features, num_classes) def forward(self, x): x=self.model(x) return x Training the Neural Network We will use the Mean Squared Error between the predicted landmarks and the true landmarks as the loss function. Keep in mind that the learning rate should be kept low to avoid exploding gradients. The network weights will be saved whenever the validation loss reaches a new minimum value. Train for at least 20 epochs to get the best performance.\nnetwork = Network() network.cuda() criterion = nn.MSELoss() optimizer = optim.Adam(network.parameters(), lr=0.0001) loss_min = np.inf num_epochs = 10 start_time = time.time() for epoch in range(1,num_epochs+1): loss_train = 0 loss_valid = 0 running_loss = 0 network.train() for step in range(1,len(train_loader)+1): images, landmarks = next(iter(train_loader)) images = images.cuda() landmarks = landmarks.view(landmarks.size(0),-1).cuda() predictions = network(images) # clear all the gradients before calculating them optimizer.zero_grad() # find the loss for the current step loss_train_step = criterion(predictions, landmarks) # calculate the gradients loss_train_step.backward() # update the parameters optimizer.step() loss_train += loss_train_step.item() running_loss = loss_train/step print_overwrite(step, len(train_loader), running_loss, \u0026#39;train\u0026#39;) network.eval() with torch.no_grad(): for step in range(1,len(valid_loader)+1): images, landmarks = next(iter(valid_loader)) images = images.cuda() landmarks = landmarks.view(landmarks.size(0),-1).cuda() predictions = network(images) # find the loss for the current step loss_valid_step = criterion(predictions, landmarks) loss_valid += loss_valid_step.item() running_loss = loss_valid/step print_overwrite(step, len(valid_loader), running_loss, \u0026#39;valid\u0026#39;) loss_train /= len(train_loader) loss_valid /= len(valid_loader) print(\u0026#39;\\n--------------------------------------------------\u0026#39;) print(\u0026#39;Epoch: {} Train Loss: {:.4f} Valid Loss: {:.4f}\u0026#39;.format(epoch, loss_train, loss_valid)) print(\u0026#39;--------------------------------------------------\u0026#39;) if loss_valid \u0026lt; loss_min: loss_min = loss_valid torch.save(network.state_dict(), \u0026#39;/content/face_landmarks.pth\u0026#39;) print(\u0026#34;\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\u0026#34;.format(loss_min, epoch, num_epochs)) print(\u0026#39;Model Saved\\n\u0026#39;) print(\u0026#39;Training Complete\u0026#39;) print(\u0026#34;Total Elapsed Time : {} s\u0026#34;.format(time.time()-start_time)) Predict on Unseen Data Use the code snippet below to predict landmarks in unseen images.\nimport time import cv2 import os import numpy as np import matplotlib.pyplot as plt from PIL import Image import imutils import torch import torch.nn as nn from torchvision import models import torchvision.transforms.functional as TF ####################################################################### image_path = \u0026#39;pic.jpg\u0026#39; weights_path = \u0026#39;face_landmarks.pth\u0026#39; frontal_face_cascade_path = \u0026#39;haarcascade_frontalface_default.xml\u0026#39; ####################################################################### class Network(nn.Module): def __init__(self,num_classes=136): super().__init__() self.model_name=\u0026#39;resnet18\u0026#39; self.model=models.resnet18(pretrained=False) self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) self.model.fc=nn.Linear(self.model.fc.in_features,num_classes) def forward(self, x): x=self.model(x) return x ####################################################################### face_cascade = cv2.CascadeClassifier(frontal_face_cascade_path) best_network = Network() best_network.load_state_dict(torch.load(weights_path, map_location=torch.device(\u0026#39;cpu\u0026#39;))) best_network.eval() image = cv2.imread(image_path) grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) display_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) height, width,_ = image.shape faces = face_cascade.detectMultiScale(grayscale_image, 1.1, 4) all_landmarks = [] for (x, y, w, h) in faces: image = grayscale_image[y:y+h, x:x+w] image = TF.resize(Image.fromarray(image), size=(224, 224)) image = TF.to_tensor(image) image = TF.normalize(image, [0.5], [0.5]) with torch.no_grad(): landmarks = best_network(image.unsqueeze(0)) landmarks = (landmarks.view(68,2).detach().numpy() + 0.5) * np.array([[w, h]]) + np.array([[x, y]]) all_landmarks.append(landmarks) plt.figure() plt.imshow(display_image) for landmarks in all_landmarks: plt.scatter(landmarks[:,0], landmarks[:,1], c = \u0026#39;c\u0026#39;, s = 5) plt.show() ⚠️ The above code snippet will not work in Colab Notebook as some functionality of the OpenCV is not supported in Colab yet. To run the above cell, use your local machine.\nOpenCV Harr Cascade Classifier is used to detect faces in an image. Object detection using Haar Cascades is a machine learning-based approach where a cascade function is trained with a set of input data. OpenCV already contains many pre-trained classifiers for face, eyes, pedestrians, and many more. In our case, we will be using the face classifier for which you need to download the pre-trained classifier XML file and save it to your working directory.\nDetected faces in the input image are then cropped, resized to (224, 224) and fed to our trained neural network to predict landmarks in them.\nThe predicted landmarks in the cropped faces are then overlayed on top of the original image. The result is the image shown below. Pretty impressive, right!\nSimilarly, landmarks detection on multiple faces:\nHere, you can see that the OpenCV Harr Cascade Classifier has detected multiple faces including a false positive (a fist is predicted as a face). So, the network has plotted some landmarks on that.\nThat’s all folks! If you made it till here, hats off to you! You just trained your very own neural network to detect face landmarks in any image. Try predicting face landmarks on your webcam feed!!\nColab Notebook The complete code can be found in the interactive Colab Notebook.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/blog/face-landmarks-detection/","summary":"Introduction Ever wondered how Instagram applies stunning filters to your face? The software detects key points on your face and projects a mask on top. This tutorial will guide you on how to build one such software using Pytorch.\nDataset In this tutorial, we will use the official DLib Dataset which contains 6666 images of varying dimensions. Additionally, labels_ibug_300W_train.xml (comes with the dataset) contains the coordinates of 68 landmarks for each face.","title":"Face Landmarks Detection using CNN"},{"content":"Introduction to machine learning In the traditional hard-coded approach, we program a computer to perform a certain task. We tell it exactly what to do when it receives a certain input. In mathematical terms, this is like saying that we write the f(x) such that when users feed the input x into f(x), it gives the correct output y.\nIn machine learning, however, we have a large set of inputs x and corresponding outputs y but not the function f(x). The goal here is to find the f(x) that transforms the input x into the output y. Well, that’s not an easy job. In this article, we will learn how this happens.\nDataset To visualize the dataset, let’s make our synthetic dataset where each data point (input x) is 3 dimensional, making it suitable to be plotted on a 3D chart. We will generate 250 points (cluster 0) in a cluster centered at the origin (0, 0, 0). A similar cluster of 250 points (cluster 1) is generated but not centered at the origin. Both clusters are relatively close but there is a clear separation as seen in the image below. These two clusters are the two classes of data points. The big green dot represents the centroid of the whole dataset.\nAfter generating the dataset, we will normalize it by subtracting the mean and dividing by the standard deviation. This is done to zero-center the data and map values in each dimension in the dataset to a common scale. This speeds up the learning.\nThe data will be saved in an array X containing the 3D coordinates of normalized points. We will also generate an array Y with the value either 0 or 1 at each index depending on which cluster the 3D point belongs.\nLearnable Function Now that we have our data ready, we can say that we have the x and y. We know that the dataset is linearly separable implying that there is a plane that can divide the dataset into the two clusters, but we don’t know what the equation of such an optimal plane is. For now, let’s just take a random plane.\nThe function f(x) should take a 3D coordinate as input and output a number between 0 and 1. If this number is less than 0.5, this point belongs to cluster 0 otherwise, it belongs to cluster 1. Let’s define a simple function for this task.\nx: input tensor of shape (num_points, 3)W: Weight (parameter) of shape (3, 1) chosen randomlyB: Bias (parameter) of shape (1, 1) chosen randomlySigmoid: A function that maps values between 0 and 1\nLet’s take a moment to understand what this function means. Before applying the sigmoid function, we are simply creating a linear mapping from the 3D coordinate (input) to 1D output. Therefore, this function will squish the whole 3D space onto a line meaning that each point in the original 3D space will now be lying somewhere on this line. Since this line will extend to infinity, we map it to [0, 1] using the Sigmoid function. As a result, for each given input, f(x) will output a value between 0 and 1.\nRemember that W and B are chosen randomly and so the 3D space will be squished onto a random line. The decision boundary for this transformation is the set of points that make f(x) = 0.5. Think why! As the 3D space is being squished onto a 1D line, a whole plane is mapped to the value 0.5 on the line. This plane is the decision boundary for f(x). Ideally, it should divide the dataset into two clusters but since W and B are randomly chosen, this plane is randomly oriented as shown below.\nOur goal is to find the right values for W and B that orients this plane (decision boundary) in such a way that it divides the dataset into the two clusters. This when done, yields a plane as shown below.\nLoss So, we are now at the starting point (random decision boundary) and we have defined the goal. We need a metric to decide how far we are from the goal. The output of the classifier is a tensor of shape (num_points, 1) where each value is between [0, 1]. If you think carefully, these values are just the probabilities of the points belonging to cluster 1. So, we can say that:\nf(x) = P(x belongs to cluster 1) 1-f(x) = P(x belongs to cluster 0) It wouldn’t be wrong to say that [1-f(x), f(x)] forms a probability distribution over the clusters 0 and cluster 1 respectively. This is the predicted probability distribution. We know for sure which cluster every point in the dataset belongs to (from y). So, we also have the true probability distribution as:\n[0, 1] when x belongs to the cluster 1 [1, 0] when x belongs to the cluster 0 A good metric to calculate the incongruity between two probability distributions is the Cross-Entropy function. As we are dealing with just 2 classes, we can use Binary Cross-Entropy (BCE). This function is available in PyTorch’s torch.nn module. If the predicted probability distribution is very similar to the true probability distribution, this function returns a small value and vice versa. We can average this value for all the data points and use it as a parameter to test how the classifier is performing.\nThis value is called the loss and mathematically, our goal now is to minimize this loss.\nTraining Now that we have defined our goal mathematically, how do we reach our goal practically? In other words, how do we find optimal values for W and B? To understand this, we will take a look at some basic calculus. Recall that we currently have random values for W and B. The process of learning or training or reaching the goal or minimizing the loss can be divided into two steps:\nForward-propagation: We feed the dataset through the classifier f(x) and use BCE to find the loss. Backpropagation: Using the loss, adjust the values of W and B to minimize the loss. The above two steps will be repeated over and over again until the loss stops decreasing. In this condition, we say that we have reached the goal!\nBackpropagation Forward propagation is simple and already discussed above. However, it is essential to take a moment to understand backpropagation as it is the key to machine learning. Recall that we have 3 parameters (variables) in W and 1 in B. So, in total, we have 4 values to optimize.\nOnce we have the loss from forward-propagation, we will calculate the gradients of the loss function with respect to each variable in the classifier. If we plot the loss for different values of each parameter, we can see that the loss is minimum at a particular value for each parameter. I have plotted the loss vs parameter for each parameter.\nAn important observation to make here is that the loss is minimized at a particular value for each of these parameters as shown by the red dot.\nLet’s consider the first plot and discuss how w1 will be optimized. The process remains the same for the other parameters. Initially, the values for W and B are chosen randomly and so (w1, loss) will be randomly placed on this curve as shown by the green dot.\nNow, the goal is to reach the red dot, starting from the green dot. In other words, we need to move downhill. Looking at the slope of the curve at the green dot, we can tell that increasing w1 (moving right) will lower the loss and therefore move the green dot closer to the red one. In mathematical terms, if the gradient of the loss with respect to w1 is negative, increase w1 to move downhill and vice versa. Therefore, w1 should be updated as:\nThe equation above is known as gradient descent equation. Here, the learning_rate controls how much we want to increase or decrease w1. If the learning_rate is large, the update will be large. This could lead to w1 going past the red dot and therefore missing the optimal value. If this value is too small, it will take forever for w1 to reach the red dot. You can try experimenting with different values of learning rate to see which works the best. In general, small values like 0.01 works well for most cases.\nIn most cases, a single update is not enough to optimize these parameters; so, the process of forward-propagation and backpropagation is repeated in a loop until the loss stops reducing further. Let’s see this in action:\nAn important observation to make is that initially the green dot moves quickly and slows down as it gradually approaches the minima. The large slope (gradient) during the first few epochs (when the green dot is far from the minima) is responsible for this large update to the parameters. The gradient decreases as the green dot approaches the minima and thus the update becomes slow. The other three parameters are trained in parallel in the exact same way. Another important observation is that the shape of the curve changes with epoch. This is due to the fact that the other three parameters (w2, w3, b) are also being updated in parallel and each parameter contributes to the shape of the loss curve.\nVisualize Let’s see how the decision boundary updates in real-time as the parameters are being updated.\nThat’s all folks! If you made it till here, hats off to you! In this article, we took a visual approach to understand how machine learning works. So far, we have seen how a simple 3D to 1D mapping, f(x), can be used to fit a decision boundary (2D plane) to a linearly separable dataset (3D). We discussed how forward propagation is used to calculate the loss followed by backpropagation where gradients of the loss with respect to parameters are calculated and the parameters are updated repeatedly in a training loop.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/blog/machine-learning-visualized/","summary":"Introduction to machine learning In the traditional hard-coded approach, we program a computer to perform a certain task. We tell it exactly what to do when it receives a certain input. In mathematical terms, this is like saying that we write the f(x) such that when users feed the input x into f(x), it gives the correct output y.\nIn machine learning, however, we have a large set of inputs x and corresponding outputs y but not the function f(x).","title":"Machine Learning - Visualized"},{"content":"Introduction In this article, we will learn how PCA can be used to compress a real-life dataset. We will be working with Labelled Faces in the Wild (LFW), a large scale dataset consisting of 13233 human-face grayscale images, each having a dimension of 64x64. It means that the data for each face is 4096 dimensional (there are 64x64 = 4096 unique values to be stored for each face). We will reduce this dimension requirement, using PCA, to just a few hundred dimensions!\nPrincipal Component Analysis (PCA) Principal component analysis (PCA) is a technique for reducing the dimensionality of datasets, exploiting the fact that the images in these datasets have something in common. For instance, in a dataset consisting of face photographs, each photograph will have facial features like eyes, nose, mouth. Instead of encoding this information pixel by pixel, we could make a template of each type of these features and then just combine these templates to generate any face in the dataset. In this approach, each template will still be 64x64 = 4096 dimensional, but since we will be reusing these templates (basis functions) to generate each face in the dataset, the number of templates required will be small. PCA does exactly this. Let’s see how!\nDataset Let’s visualize some images from the dataset. You can see that each image has a complete face, and the facial features like eyes, nose, and lips are clearly visible in each image. Now that we have our dataset ready, let’s compress it.\nCompression PCA is a 4 step process. Starting with a dataset containing n dimensions (requiring n-axes to be represented):\nStep 1: Find a new set of basis functions (naxes) where some axes contribute to most of the variance in the dataset while others contribute very little. Step 2: Arrange these axes in the decreasing order of variance contribution. Step 3: Now, pick the top k axes to be used and drop the remaining n-k axes. Step 4: Now, project the dataset onto these k axes. These steps are well explained in my previous article. After these 4 steps, the dataset will be compressed from n-dimensions to just k-dimensions (k\u0026lt;n).\nStep 1 Finding a new set of basis functions (n-axes), where some axes contribute to most of the variance in the dataset while others contribute very little, is analogous to finding the templates that we will combine later to generate faces in the dataset. A total of 4096 templates, each 4096 dimensional, will be generated. Each face in the dataset can be represented as a linear combination of these templates.\nPlease note that the scalar constants (k1, k2, …, kn) will be unique for each face.\nStep 2 Now, some of these templates contribute significantly to facial reconstruction while others contribute very little. This level of contribution can be quantified as the percentage of variance that each template contributes to the dataset. So, in this step, we will arrange these templates in the decreasing order of variance contribution (most significant…least significant).\nStep 3 Now, we will keep the top k templates and drop the remaining. But, how many templates shall we keep? If we keep more templates, our reconstructed images will closely resemble the original images but we will need more storage to store the compressed data. If we keep too few templates, our reconstructed images will look very different from the original images.\nThe best solution is to fix the percentage of variance that we want to retain in the compressed dataset and use this to determine the value of k (number of templates to keep). If we do the math, we find that to retain 99% of the variance, we need only the top 577 templates. We will save these values in an array and drop the remaining templates.\nLet’s visualize some of these selected templates.\nPlease note that each of these templates looks somewhat like a face. These are called as Eigenfaces.\nStep 4 Now, we will construct a projection matrix to project the images from the original 4096 dimensions to just 577 dimensions. The projection matrix will have a shape (4096, 577), where the templates will be the columns of the matrix.\nBefore we go ahead and compress the images, let’s take a moment to understand what we really mean by compression. Recall that the faces can be generated by a linear combination of the selected templates. As each face is unique, every face in the dataset will require a different set of constants (k1, k2, …, kn) for the linear combination.\nLet’s start with an image from the dataset and compute the constants (k1, k2, …, kn), where n = 577. These constants along with the selected 577 templates can be plugged in the equation above to reconstruct the face. This means that we only need to compute and save these 577 constants for each image. Instead of doing this image by image, we can use matrices to compute these constants for each image in the dataset at the same time.\nRecall that there are 13233 images in the dataset. The matrix compressed_images contains the 577 constants for each image in the dataset. We can now say that we have compressed our images from 4096 dimensions to just 577 dimensions while retaining 99% of the information.\nCompression Ratio Let’s calculate how much we have compressed the dataset. Recall that there are 13233 images in the dataset and each image is 64x64 dimensional. So, the total number of unique values required to store the original dataset is13233 x 64 x 64 = 54,202,368 unique values.\nAfter compression, we store 577 constants for each image. So, the total number of unique values required to store the compressed dataset is13233 x 577 = 7,635,441 unique values. But, we also need to store the templates to reconstruct the images later. Therefore, we also need to store577 x 64 x 64 = 2,363,392 unique values for the templates. Therefore, the total number of unique values required to store the compressed dataset is7,635,441 + 2,363,392 = 9,998,883 unique values.\nWe can calculate the percentage compression as:\nReconstruct the Images The compressed images are just arrays of length 577 and can’t be visualized as such. We need to reconstruct it back to 4096 dimensions to view it as an array of shape (64x64). Recall that each template has a dimension of 64x64 and that each constant is a scalar value. We can use the equation below to reconstruct any face in the dataset.\nAgain, instead of doing this image by image, we can use matrices to reconstruct the whole dataset at once, with of course a loss of 1% variance.\nLet’s look at some reconstructed faces.\nWe can see that the reconstructed images have captured most of the relevant information about the faces and the unnecessary details have been ignored. This is an added advantage of data compression, it allows us to filter unnecessary details (and even noise) present in the data.\nThat’s all folks! If you made it till here, hats off to you! In this article, we learnt how PCA can be used to compress Labelled Faces in the Wild (LFW), a large scale dataset consisting of 13233 human-face images, each having a dimension of 64x64. We compressed this dataset by over 80% while retaining 99% of the information.\nColab Notebook View my Colab Notebook for a well commented code!\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/blog/face-dataset-compression/","summary":"Introduction In this article, we will learn how PCA can be used to compress a real-life dataset. We will be working with Labelled Faces in the Wild (LFW), a large scale dataset consisting of 13233 human-face grayscale images, each having a dimension of 64x64. It means that the data for each face is 4096 dimensional (there are 64x64 = 4096 unique values to be stored for each face). We will reduce this dimension requirement, using PCA, to just a few hundred dimensions!","title":"Face Dataset Compression using PCA"},{"content":"Introduction If you have ever taken an online course on Machine Learning, you must have come across Principal Component Analysis for dimensionality reduction, or in simple terms, for compression of data. Guess what, I had taken such courses too but I never really understood the graphical significance of PCA because all I saw was matrices and equations. It took me quite a lot of time to understand this concept from various sources. So, I decided to compile it all in one place.\nIn this article, we will take a visual (graphical) approach to understand PCA and how it can be used to compress data. Basic knowledge of Linear Algebra and Matrices is assumed. If you are new to this concept, just follow along, I have tried my best to keep this as simple as possible.\nThese days, datasets containing a large number of dimensions are increasingly common and are often difficult to interpret. One example can be a database of face photographs of let’s say, 1,000,000 people. If each face photograph has a dimension of 100x100, then the data of each face is 10000 dimensional (there are 100x100 = 10,000 unique values to be stored for each face). Now, if 1 byte is required to store the information of each pixel, then 10,000 bytes are required to store 1 face. Since there are 1000 faces in the database,10,000 x 1,000,000 = 10 GB will be needed to store the dataset.\nPrincipal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, exploiting the fact that the images in these datasets have something in common. For instance, in a dataset consisting of face photographs, each photograph will have facial features like eyes, nose, mouth. Instead of encoding this information pixel by pixel, we could make a template of each type of these features and then just combine these templates to generate any face in the dataset. In this approach, each template will still be 100x100 = 1000 dimensional, but since we will be reusing these templates (basis functions) to generate each face in the dataset, the number of templates required will be very small. PCA does exactly this.\nHow does PCA work? This part is going to be a bit technical, so bear with me! I will try to explain the working of PCA with a simple example. Let’s consider the data shown below containing 100 points each 2 dimensional (x \u0026amp; y coordinates is needed to represent each point).\nCurrently, we are using 2 values to represent each point. Let’s explain this situation in a more technical way. We are currently using 2 basis functions,x as (1, 0) and y as (0, 1). Each point in the dataset is represented as a weighted sum of these basis functions. For instance, point (2, 3) can be represented as 2(1, 0) + 3(0, 1) = (2, 3). If we omit either of these basis functions, we will not be able to represent the points in the dataset accurately. Therefore, both the dimensions necessary, and we can’t just drop one of them to reduce the storage requirement. This set of basis functions is actually the cartesian coordinate in 2 dimensions.\nIf we notice closely, we can very well see that the data approximates a line as shown by the red line below.\nNow, let’s rotate the coordinate system such that the x-axis lies along the red line. Then, the y-axis (green line) will be perpendicular to this red line. Let’s call these new x and y axes as a-axis and b-axis respectively. This is shown below.\nNow, if we use a and b as the new set basis functions (instead of using x and y) for this dataset, it wouldn’t be wrong to say that most of the variance in the dataset is along the a-axis. Now, if we drop the b-axis, we can still represent the points in the dataset very accurately, using just a-axis. Therefore, we now only need half as must storage to store the dataset and reconstruct it accurately. This is exactly how PCA works.\nPCA is a 4 step process. Starting with a dataset containing n dimensions (requiring n-axes to be represented):\nFind a new set of basis functions (naxes) where some axes contribute to most of the variance in the dataset while others contribute very little. Arrange these axes in the decreasing order of variance contribution. Now, pick the top k axes to be used and drop the remaining n-k axes. Now, project the dataset onto these k axes. After these 4 steps, the dataset will be compressed from n-dimensions to just k-dimensions (k\u0026lt;n).\nSteps For the sake of simplicity, let’s take the above dataset and apply PCA on that. The steps involved will be technical and basic knowledge of linear algebra is assumed.\nStep 1 Since this is a 2-dimensional dataset, n=2. The first step is to find the new set of basis functions (a \u0026amp; b). In the explanation above, we saw that the dataset had the maximum variance along a line and we manually chose that line as a-axis and the line the perpendicular to it as b-axis. In practice, we want this step to be automated.\nTo accomplish this, we can find the eigenvalues and eigenvectors of the covariance matrix of the dataset. Since the dataset is 2 dimensional, we will get 2 eigenvalues and their corresponding eigenvectors. Then, the 2 eigenvectors are two basis functions (new axes) and the two eigenvalues tell us the variance contribution of the corresponding eigenvectors. A large value of eigenvalue implies that the corresponding eigenvector (axis) contributes more towards the total variance of the dataset.\nStep 2 Now, sort the eigenvectors (axes) according to decreasing eigenvalues. Here, we can see that the eigenvalue for a-axis is much larger than that of theb-axis meaning that a-axis contributes more towards the dataset variance.\nThe percentage contribution of each axis towards the total dataset variance can be calculated as:\nThe above numbers prove that the a-axis contributes 99.7% towards the dataset variance and that we can drop the b-axis and lose just 0.28% of the variance.\nStep 3 Now, we will drop the b-axis and keep only the a-axis.\nStep 4 Now, reshape the first eigenvector (a-axis) into a 2x1 matrix, called the projection matrix. It will be used to project the original dataset of shape(100, 2) onto the new basis function (a-axis), thus compressing it to (100, 1).\nReconstruct the data Now, we can use the projection matrix to expand the data back to its original size, with of course a small loss of variance (0.28%).\nThe reconstructed data is shown below:\nPlease note that the variance along the b-axis (0.28%) is lost as evident by the above figure.\nThat’s all folks! If you made it till here, hats off to you! In this article, we took a graphical approach to understand how Principal Component Analysis works and how it can be used for data compression.\nColab Notebook View my Colab Notebook for a well commented code!\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/blog/pca-visualized/","summary":"Introduction If you have ever taken an online course on Machine Learning, you must have come across Principal Component Analysis for dimensionality reduction, or in simple terms, for compression of data. Guess what, I had taken such courses too but I never really understood the graphical significance of PCA because all I saw was matrices and equations. It took me quite a lot of time to understand this concept from various sources.","title":"Principal Component Analysis - Visualized"},{"content":"Intro In my video about \u0026lt;strong\u0026gt;How I cleared the AWS SAA Certification Exam\u0026lt;/strong\u0026gt;, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac. Once I\u0026rsquo;m done editing my notes, I push them to a GitHub repository to make sure I don\u0026rsquo;t lose them if my laptop breaks.\nSo, if you want to view my notes exactly like I do, you can clone my \u0026lt;strong\u0026gt;Obsidian Vault\u0026lt;/strong\u0026gt; repository and download Obsidian to render it. But, this solution isn\u0026rsquo;t elegant as it would require you to download an additional software. So, I along with my college roommate, \u0026lt;strong\u0026gt;Sarthak Narayan\u0026lt;/strong\u0026gt;, had been working over the past 2 weeks on the project, \u0026lt;strong\u0026gt;Obsidian Publish using GitHub Action\u0026lt;/strong\u0026gt;, which would allow us to effortlessly publish our notes as a static website.\nIt is complete and I\u0026rsquo;ve used it to publish my notes at \u0026lt;strong\u0026gt;notes.arkalim.org\u0026lt;/strong\u0026gt;. Working The GitHub Action spins up a Docker container which parses and converts Obsidian markdown notes into a special markdown format understood by MkDocs, an open-source static site generator. MkDocs is actually meant for preparing documentations but works well for notes too. After the markdown files have been converted, all the images in my notes are compressed to a fraction of their original size so that they can load quickly in your web browser. A static site is then built using MkDocs and then finally deployed on Netlify. All of this happens automatically using GitHub Actions. All I have to do is update my notes and push the changes to GitHub.\nFinal thoughts Having an automated way to publish your notes online with the community is a powerful way to share knowledge. This project has also made it exceedingly easy for me to refer my notes from anywhere, which is powerful when you work on a lot of systems.\nResources My Notes Obsidian Publish - GitHub Action Parser and Image Compressor MkDocs - Material Theme daddw\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/theme-project/obsidian-publish-github-action/","summary":"Intro In my video about \u0026lt;strong\u0026gt;How I cleared the AWS SAA Certification Exam\u0026lt;/strong\u0026gt;, I shared my preparation strategy as well as tips to ace the exam. I also gave a glimpse of my revision notes that I prepared while taking the course and practice exams on Udemy. After that video was out, I got so many comments and DMs, requesting me to share my notes, but the problem was that I took these notes using a note-taking app called Obsidian which stores them in markdown format locally on my Mac.","title":"Obsidian Publish using GitHub Action"},{"content":"🔗 GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users don’t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/theme-project/kindle-to-notion/","summary":"🔗 GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.","title":"Kindle to Notion"},{"content":"🔗 View App 🔗 GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/theme-project/todo-list-app/","summary":"🔗 View App 🔗 GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.","title":"Todo List App"},{"content":"🔗 Colab Notebook 🔗 Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/theme-project/face-landmarks-detection/","summary":"🔗 Colab Notebook 🔗 Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better.","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/theme-project/gaze-tracking-goggles/","summary":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre.","title":"Gaze-tracking Goggles"},{"content":"🔗 GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/theme-project/openquad/","summary":"🔗 GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\n🔗 Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body can’t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/theme-project/search-and-reconnaissance-robot/","summary":"Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\n🔗 Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris.","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/products/theme-project/sebart-pro/","summary":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter.","title":"SEBART-Pro"},{"content":"Description Integrated Rho product with GE Healthcare’s Edison platform which is expected to significantly increase the adoption of Rho among Canadian hospitals. GE Healthcare provided Amazon EKS to deploy Rho, which was originally designed to work on Docker-Compose. As a part of this integration, I wrote Kubernetes manifests to migrate Rho from Docker-Compose to Kubernetes. Automated integration testing of all the major backend workflows saving more than 2h of weekly testing time. Asynchronously decoupled individual micro-services using RabbitMQ and implemented dead letter queues (DLQs) for each queue to ensure retry of failed messages. Wrote Python scripts to automate installation and updation of Rho on customer site. Wrote bash scripts to automate the backup and restore functionality of Rho. ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/experience/16bit/","summary":"Description Integrated Rho product with GE Healthcare’s Edison platform which is expected to significantly increase the adoption of Rho among Canadian hospitals. GE Healthcare provided Amazon EKS to deploy Rho, which was originally designed to work on Docker-Compose. As a part of this integration, I wrote Kubernetes manifests to migrate Rho from Docker-Compose to Kubernetes. Automated integration testing of all the major backend workflows saving more than 2h of weekly testing time.","title":"DevOps Intern"},{"content":"Description Developed an event-driven serverless integration framework using AWS services like AppFlow, S3, Lambda and EventBridge, to sync customer data between Salesforce and BuyerAssist. Through this, I learned to build systems to support bi-directional sync of large volumes of data from multiple sources, perform CRUD operations on MongoDB as well as database schema design. Developed a configuration-driven framework to extend the pattern matching capability of AWS EventBridge, which prevented thousands of false invocations of AWS Lambda functions. Implemented a system to track asynchronous data transfer jobs through AWS AppFlow, which reduced the issue tracking time to under 5 mins. Developed a Salesforce app using SFDX to provide clients with a customized experience within their Salesforce dashboard. Developed a Slack bot to send interactive daily notifications to customers, and to allow them to take actions directly from Slack. This eliminated the operational resistance and increased the adoption of our product by over 50%. Implemented authorization for Slack integration with BuyerAssist using React and OAuth 2.0 Mentored a new recruit for a period of 1 month ","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/experience/buyerassist/","summary":"Description Developed an event-driven serverless integration framework using AWS services like AppFlow, S3, Lambda and EventBridge, to sync customer data between Salesforce and BuyerAssist. Through this, I learned to build systems to support bi-directional sync of large volumes of data from multiple sources, perform CRUD operations on MongoDB as well as database schema design. Developed a configuration-driven framework to extend the pattern matching capability of AWS EventBridge, which prevented thousands of false invocations of AWS Lambda functions.","title":"Backend Engineer"},{"content":"🔗 GitHub Description Guide: Mohammad Farid Azampour (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\nMy work focused on using Pix2Pix (a CGAN architecture) to generate Ultrasound (US) scans from MRI scans, an image-to-image translation problem. However, a major challenge that I faced was the lack of structural correspondence between the MRI and US scans, arising from the sheer nature of the way this data is collected. Consequently, I wrote a custom loss function incorporating the CGAN loss with a Dice Loss between the segmentation maps obtained from the MRI scans and those from the generated US scan. This forces the generator to remove the structural deformation in the generated US scans. Additionally, I was given remote access to the TU-Munich’s cluster computers for training the model as well as an account in their Discourse forum.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/experience/tumunich/","summary":"🔗 GitHub Description Guide: Mohammad Farid Azampour (Visiting Researcher at Chair for Computer Aided Medical Procedures, TU Munich)\nMy work focused on using Pix2Pix (a CGAN architecture) to generate Ultrasound (US) scans from MRI scans, an image-to-image translation problem. However, a major challenge that I faced was the lack of structural correspondence between the MRI and US scans, arising from the sheer nature of the way this data is collected. Consequently, I wrote a custom loss function incorporating the CGAN loss with a Dice Loss between the segmentation maps obtained from the MRI scans and those from the generated US scan.","title":"Remote Research Intern"},{"content":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nI worked as a remote intern at \u0026lt;strong\u0026gt;OriginHealth Pte. Ltd.\u0026lt;/strong\u0026gt; (Singapore), where the company envisions to automate the detection of birth defects from fetal ultrasound scans, a procedure that demands expertise in radiology.\nHere, I developed a configuration-driven framework with data preprocessing pipeline to train deep learning models on AWS EC2 instances. The framework standardized the training procedure for ML models and saved more than 100 hours of development time for the ML team. I also worked on fetal head segmentation using this framework.\nThis opportunity provided me with a deeper insight into the applications of AI and Computer Vision in medical diagnosis and taught me to work with little and sensitive data. In addition, Confluence was used for documentation and Jira Software was used for Agile Software Development.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/experience/origin-health/","summary":"Description Guide: Dr. Sripad Krishna Devalla (co-founder and CTO at OriginHealth)\nI worked as a remote intern at \u0026lt;strong\u0026gt;OriginHealth Pte. Ltd.\u0026lt;/strong\u0026gt; (Singapore), where the company envisions to automate the detection of birth defects from fetal ultrasound scans, a procedure that demands expertise in radiology.\nHere, I developed a configuration-driven framework with data preprocessing pipeline to train deep learning models on AWS EC2 instances. The framework standardized the training procedure for ML models and saved more than 100 hours of development time for the ML team.","title":"Software Intern"},{"content":"🔗 GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nDuring my internship, I worked under the guidance of Prof. Pratyush Kumar (Assistant Professor, Department of Computer Science, IIT Madras) where I implemented a Convolutional Neural Network for 6-DoF Global Pose Regression and Odometry Estimation from consecutive monocular images. The model estimates the camera pose from a sequence of monocular images from the camera. At each step, the model takes two consecutive frames as input and returns the global and relative pose between the two frames. It was built and trained from scratch in Tensorflow and it outperformed traditional feature-based visual localization algorithms, especially in texture-less regions. The neural network was later used by Prof. Pratyush for the localization of robots in GPS denied environments.\n","permalink":"https://nikhil9237.github.io/nikhilkakade.github.io/experience/iit-madras/","summary":"🔗 GitHub Description Guide: Prof. Dr. Pratyush Kumar (Assistant Professor, Dept. of Computer Science, IIT Madras)\nDuring my internship, I worked under the guidance of Prof. Pratyush Kumar (Assistant Professor, Department of Computer Science, IIT Madras) where I implemented a Convolutional Neural Network for 6-DoF Global Pose Regression and Odometry Estimation from consecutive monocular images. The model estimates the camera pose from a sequence of monocular images from the camera. At each step, the model takes two consecutive frames as input and returns the global and relative pose between the two frames.","title":"Computer Vision Intern"}]